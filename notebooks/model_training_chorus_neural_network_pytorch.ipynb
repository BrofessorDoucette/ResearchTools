{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# caution: path[0] is reserved for script path (or '' in REPL).\n",
    "sys.path.insert(1, os.path.abspath('./../src'))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChorusNeuralNetworkDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_version, is_validation = False):\n",
    "        \n",
    "        self.dataset_version = dataset_version\n",
    "\n",
    "        CONJUNCTIONS_REFS = np.load(f\"./../processed_data_chorus_neural_network/STAGE_4/{dataset_version}/MODEL_READY_DATA_{dataset_version}.npz\")\n",
    "\n",
    "        if is_validation:\n",
    "            \n",
    "            self.features = CONJUNCTIONS_REFS[\"VALIDATION_FEATURES\"].astype(np.float32)\n",
    "            self.labels = CONJUNCTIONS_REFS[\"VALIDATION_LABELS\"].astype(np.float32)\n",
    "        else:\n",
    "            \n",
    "            self.features = CONJUNCTIONS_REFS[\"FEATURES\"].astype(np.float32)\n",
    "            self.labels = CONJUNCTIONS_REFS[\"LABELS\"].astype(np.float32)\n",
    "                    \n",
    "        CONJUNCTIONS_REFS.close()\n",
    "        \n",
    "    def get_bins(self):\n",
    "        \n",
    "        return self.BINS\n",
    "        \n",
    "    # get sample\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return torch.from_numpy(self.features[idx, :]), torch.from_numpy(self.labels[idx, :])\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChorusNeuralNetwork(\n",
      "  (RELU_STACK_THEN_LINEAR): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=15, out_features=1024, bias=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "    (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (4): PReLU(num_parameters=1)\n",
      "    (5): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters in model: 279043\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class ChorusNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.RELU_STACK_THEN_LINEAR = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(15, 1024),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.RELU_STACK_THEN_LINEAR(x)\n",
    "        return logits\n",
    "\n",
    "model = ChorusNeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 345313\n",
      "Validation set size: 9138\n"
     ]
    }
   ],
   "source": [
    "dataset = ChorusNeuralNetworkDataset(\"v4a\")\n",
    "validation_set = ChorusNeuralNetworkDataset(\"v4a\", is_validation=True)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f\"Training set size: {len(dataset)}\")\n",
    "print(f\"Validation set size: {len(validation_set)}\")\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    ")\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(\n",
    "            validation_set,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.SmoothL1Loss(\"mean\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"training loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum learning rate: 0.001\n",
      "Beginning training for epoch: 0!\n",
      "training loss: 0.606920  [   64/345313]\n",
      "training loss: 0.136775  [64064/345313]\n",
      "training loss: 0.198092  [128064/345313]\n",
      "training loss: 0.267768  [192064/345313]\n",
      "training loss: 0.158098  [256064/345313]\n",
      "training loss: 0.149851  [320064/345313]\n",
      "Beginning validation testing for epoch: 0!\n",
      "Validation Error: \n",
      " Avg loss: 0.210179 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_0.pth\n",
      "Maximum learning rate: 0.00075\n",
      "Beginning training for epoch: 1!\n",
      "training loss: 0.218964  [   64/345313]\n",
      "training loss: 0.250755  [64064/345313]\n",
      "training loss: 0.215892  [128064/345313]\n",
      "training loss: 0.216524  [192064/345313]\n",
      "training loss: 0.155179  [256064/345313]\n",
      "training loss: 0.173043  [320064/345313]\n",
      "Beginning validation testing for epoch: 1!\n",
      "Validation Error: \n",
      " Avg loss: 0.210551 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_1.pth\n",
      "Maximum learning rate: 0.0005625000000000001\n",
      "Beginning training for epoch: 2!\n",
      "training loss: 0.171068  [   64/345313]\n",
      "training loss: 0.242033  [64064/345313]\n",
      "training loss: 0.178611  [128064/345313]\n",
      "training loss: 0.292951  [192064/345313]\n",
      "training loss: 0.262717  [256064/345313]\n",
      "training loss: 0.222438  [320064/345313]\n",
      "Beginning validation testing for epoch: 2!\n",
      "Validation Error: \n",
      " Avg loss: 0.210047 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_2.pth\n",
      "Maximum learning rate: 0.00042187500000000005\n",
      "Beginning training for epoch: 3!\n",
      "training loss: 0.231332  [   64/345313]\n",
      "training loss: 0.248849  [64064/345313]\n",
      "training loss: 0.205947  [128064/345313]\n",
      "training loss: 0.221866  [192064/345313]\n",
      "training loss: 0.183129  [256064/345313]\n",
      "training loss: 0.153423  [320064/345313]\n",
      "Beginning validation testing for epoch: 3!\n",
      "Validation Error: \n",
      " Avg loss: 0.211737 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_3.pth\n",
      "Maximum learning rate: 0.00031640625000000006\n",
      "Beginning training for epoch: 4!\n",
      "training loss: 0.215432  [   64/345313]\n",
      "training loss: 0.161187  [64064/345313]\n",
      "training loss: 0.282780  [128064/345313]\n",
      "training loss: 0.118502  [192064/345313]\n",
      "training loss: 0.210069  [256064/345313]\n",
      "training loss: 0.217611  [320064/345313]\n",
      "Beginning validation testing for epoch: 4!\n",
      "Validation Error: \n",
      " Avg loss: 0.211562 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_4.pth\n",
      "Maximum learning rate: 0.00023730468750000005\n",
      "Beginning training for epoch: 5!\n",
      "training loss: 0.188716  [   64/345313]\n",
      "training loss: 0.195853  [64064/345313]\n",
      "training loss: 0.172732  [128064/345313]\n",
      "training loss: 0.219658  [192064/345313]\n",
      "training loss: 0.165668  [256064/345313]\n",
      "training loss: 0.217195  [320064/345313]\n",
      "Beginning validation testing for epoch: 5!\n",
      "Validation Error: \n",
      " Avg loss: 0.211213 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_5.pth\n",
      "Maximum learning rate: 0.00017797851562500002\n",
      "Beginning training for epoch: 6!\n",
      "training loss: 0.171737  [   64/345313]\n",
      "training loss: 0.130203  [64064/345313]\n",
      "training loss: 0.173649  [128064/345313]\n",
      "training loss: 0.199279  [192064/345313]\n",
      "training loss: 0.227135  [256064/345313]\n",
      "training loss: 0.261350  [320064/345313]\n",
      "Beginning validation testing for epoch: 6!\n",
      "Validation Error: \n",
      " Avg loss: 0.211423 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_6.pth\n",
      "Maximum learning rate: 0.00013348388671875002\n",
      "Beginning training for epoch: 7!\n",
      "training loss: 0.129674  [   64/345313]\n",
      "training loss: 0.153659  [64064/345313]\n",
      "training loss: 0.265840  [128064/345313]\n",
      "training loss: 0.172291  [192064/345313]\n",
      "training loss: 0.209743  [256064/345313]\n",
      "training loss: 0.246034  [320064/345313]\n",
      "Beginning validation testing for epoch: 7!\n",
      "Validation Error: \n",
      " Avg loss: 0.209226 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_7.pth\n",
      "Maximum learning rate: 0.00010011291503906251\n",
      "Beginning training for epoch: 8!\n",
      "training loss: 0.238928  [   64/345313]\n",
      "training loss: 0.145992  [64064/345313]\n",
      "training loss: 0.216616  [128064/345313]\n",
      "training loss: 0.149201  [192064/345313]\n",
      "training loss: 0.143876  [256064/345313]\n",
      "training loss: 0.193480  [320064/345313]\n",
      "Beginning validation testing for epoch: 8!\n",
      "Validation Error: \n",
      " Avg loss: 0.210653 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_8.pth\n",
      "Maximum learning rate: 7.508468627929689e-05\n",
      "Beginning training for epoch: 9!\n",
      "training loss: 0.156480  [   64/345313]\n",
      "training loss: 0.179614  [64064/345313]\n",
      "training loss: 0.232908  [128064/345313]\n",
      "training loss: 0.214396  [192064/345313]\n",
      "training loss: 0.287209  [256064/345313]\n",
      "training loss: 0.196559  [320064/345313]\n",
      "Beginning validation testing for epoch: 9!\n",
      "Validation Error: \n",
      " Avg loss: 0.209156 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_9.pth\n",
      "Maximum learning rate: 5.631351470947266e-05\n",
      "Beginning training for epoch: 10!\n",
      "training loss: 0.257942  [   64/345313]\n",
      "training loss: 0.232525  [64064/345313]\n",
      "training loss: 0.145857  [128064/345313]\n",
      "training loss: 0.094949  [192064/345313]\n",
      "training loss: 0.132682  [256064/345313]\n",
      "training loss: 0.178387  [320064/345313]\n",
      "Beginning validation testing for epoch: 10!\n",
      "Validation Error: \n",
      " Avg loss: 0.210272 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_10.pth\n",
      "Maximum learning rate: 4.22351360321045e-05\n",
      "Beginning training for epoch: 11!\n",
      "training loss: 0.242032  [   64/345313]\n",
      "training loss: 0.175066  [64064/345313]\n",
      "training loss: 0.257014  [128064/345313]\n",
      "training loss: 0.227910  [192064/345313]\n",
      "training loss: 0.241932  [256064/345313]\n",
      "training loss: 0.180589  [320064/345313]\n",
      "Beginning validation testing for epoch: 11!\n",
      "Validation Error: \n",
      " Avg loss: 0.210086 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_11.pth\n",
      "Maximum learning rate: 3.167635202407837e-05\n",
      "Beginning training for epoch: 12!\n",
      "training loss: 0.173000  [   64/345313]\n",
      "training loss: 0.247480  [64064/345313]\n",
      "training loss: 0.174284  [128064/345313]\n",
      "training loss: 0.222867  [192064/345313]\n",
      "training loss: 0.207246  [256064/345313]\n",
      "training loss: 0.193568  [320064/345313]\n",
      "Beginning validation testing for epoch: 12!\n",
      "Validation Error: \n",
      " Avg loss: 0.209492 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_12.pth\n",
      "Maximum learning rate: 2.3757264018058778e-05\n",
      "Beginning training for epoch: 13!\n",
      "training loss: 0.237601  [   64/345313]\n",
      "training loss: 0.208246  [64064/345313]\n",
      "training loss: 0.224881  [128064/345313]\n",
      "training loss: 0.209505  [192064/345313]\n",
      "training loss: 0.150744  [256064/345313]\n",
      "training loss: 0.197131  [320064/345313]\n",
      "Beginning validation testing for epoch: 13!\n",
      "Validation Error: \n",
      " Avg loss: 0.209450 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_13.pth\n",
      "Maximum learning rate: 1.7817948013544083e-05\n",
      "Beginning training for epoch: 14!\n",
      "training loss: 0.228403  [   64/345313]\n",
      "training loss: 0.313369  [64064/345313]\n",
      "training loss: 0.164254  [128064/345313]\n",
      "training loss: 0.157110  [192064/345313]\n",
      "training loss: 0.188944  [256064/345313]\n",
      "training loss: 0.171789  [320064/345313]\n",
      "Beginning validation testing for epoch: 14!\n",
      "Validation Error: \n",
      " Avg loss: 0.209834 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_14.pth\n",
      "Maximum learning rate: 1.3363461010158061e-05\n",
      "Beginning training for epoch: 15!\n",
      "training loss: 0.221791  [   64/345313]\n",
      "training loss: 0.192956  [64064/345313]\n",
      "training loss: 0.211702  [128064/345313]\n",
      "training loss: 0.239853  [192064/345313]\n",
      "training loss: 0.200705  [256064/345313]\n",
      "training loss: 0.212484  [320064/345313]\n",
      "Beginning validation testing for epoch: 15!\n",
      "Validation Error: \n",
      " Avg loss: 0.208888 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_15.pth\n",
      "Maximum learning rate: 1.0022595757618546e-05\n",
      "Beginning training for epoch: 16!\n",
      "training loss: 0.161588  [   64/345313]\n",
      "training loss: 0.189399  [64064/345313]\n",
      "training loss: 0.130091  [128064/345313]\n",
      "training loss: 0.263398  [192064/345313]\n",
      "training loss: 0.214201  [256064/345313]\n",
      "training loss: 0.196840  [320064/345313]\n",
      "Beginning validation testing for epoch: 16!\n",
      "Validation Error: \n",
      " Avg loss: 0.209495 \n",
      "\n",
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_16.pth\n",
      "Maximum learning rate: 7.51694681821391e-06\n",
      "Beginning training for epoch: 17!\n",
      "training loss: 0.244009  [   64/345313]\n",
      "training loss: 0.160179  [64064/345313]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaximum learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeginning training for epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m training_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m training_losses\u001b[38;5;241m.\u001b[39mappend(training_loss)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeginning validation testing for epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[34], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 16\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     19\u001b[0m     loss, current \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem(), (batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X)\n",
      "File \u001b[1;32mc:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\torch\\optim\\adam.py:524\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling() \u001b[38;5;129;01mand\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[1;32m--> 524\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    528\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f\"Maximum learning rate: {scheduler.get_last_lr()[0]}\")\n",
    "    \n",
    "    print(f\"Beginning training for epoch: {epoch}!\")\n",
    "    training_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    training_losses.append(training_loss)\n",
    "    \n",
    "    print(f\"Beginning validation testing for epoch: {epoch}!\")\n",
    "    validation_loss = test(validation_dataloader, model, loss_fn)\n",
    "    print(f\"Validation Error: \\n Avg loss: {validation_loss:>8f} \\n\")\n",
    "    validation_losses.append(validation_loss)\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.abspath(f\"./../processed_data_chorus_neural_network/TRAINED_MODELS/Regression_with_smaller_dataset/Regression_with_smaller_dataset_{epoch}.pth\"))\n",
    "    print(f\"Saved PyTorch Model State to {os.path.abspath(f\"./../processed_data_chorus_neural_network/TRAINED_MODELS/Regression_with_smaller_dataset/Regression_with_smaller_dataset_{epoch}.pth\")}\")\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1526cea6b10>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(validation_losses)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to c:\\Dev\\Research\\REPT_Enhancements_Tool\\processed_data_chorus_neural_network\\TRAINED_MODELS\\Regression_with_smaller_dataset\\Regression_with_smaller_dataset_12.pth\n"
     ]
    }
   ],
   "source": [
    "recent_model = os.path.abspath(\"./../processed_data_chorus_neural_network/TRAINED_MODELS/Regression_with_smaller_dataset/Regression_with_smaller_dataset_12.pth\")\n",
    "\n",
    "#torch.save(model.state_dict(), recent_model)\n",
    "print(f\"Saved PyTorch Model State to {recent_model}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChorusNeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(recent_model, weights_only=True))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, y = torch.tensor(validation_set.features), validation_set.labels\n",
    "    x = x.to(device)\n",
    "    pred = np.array(model(x).cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 100.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(np.exp(validation_set.labels), np.exp(pred))\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(1e-0, 1e2)\n",
    "plt.ylim(1e-0, 1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1, 100.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, y = torch.tensor(dataset.features), dataset.labels\n",
    "    x = x.to(device)\n",
    "    pred = np.array(model(x).cpu())\n",
    "\n",
    "\n",
    "plt.scatter(np.exp(dataset.labels), np.exp(pred))\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(1e-1, 1e2)\n",
    "plt.ylim(1e-1, 1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
