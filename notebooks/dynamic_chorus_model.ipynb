{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3522b78-30e0-44b7-9298-323fa9c642f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/uxlfoundation/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# caution: path[0] is reserved for script path (or '' in REPL).\n",
    "sys.path.insert(1, os.path.abspath(\"./../src\"))\n",
    "\n",
    "import datetime\n",
    "import importlib\n",
    "\n",
    "import astropy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "import sklearn\n",
    "\n",
    "import chorus_machine_learning_helper\n",
    "import data_loader\n",
    "import plot_tools\n",
    "import dynamic_chorus_model\n",
    "\n",
    "importlib.reload(data_loader)\n",
    "importlib.reload(dynamic_chorus_model)\n",
    "importlib.reload(chorus_machine_learning_helper)\n",
    "importlib.reload(plot_tools)\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9bb1306-d3ec-47b7-9c06-b92d5cff7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"v1\"\n",
    "MODEL_TYPE = \"LOWER_BAND\"\n",
    "\n",
    "pdata_folder = os.path.abspath(\"./../processed_data/chorus_neural_network/\")\n",
    "rbsp_chorus_folder = os.path.join(pdata_folder, \"observed_chorus\")\n",
    "output_folder = os.path.join(pdata_folder, \"models\", VERSION)\n",
    "\n",
    "T_SIZE = 31860 / 2\n",
    "SAMPLING_SIZE = 100000\n",
    "year = 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed1266b-edce-40b0-9357-6ef5b40a3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "POES_temp = data_loader.load_raw_data_from_config(\n",
    "    id=[\"POES\", \"SEM\", \"MPE\"],\n",
    "    start=datetime.datetime(year=2000, month=1, day=1),\n",
    "    end=datetime.datetime(year=2000, month=1, day=2),\n",
    "    satellite=\"n15\",\n",
    ")\n",
    "\n",
    "ENERGIES = POES_temp[\"energy\"][0]\n",
    "DIFF_E = ENERGIES[1:] - ENERGIES[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cb1cb-f868-4915-b7a4-96a0f626518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POES PREPROCESSING\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b714ceae-29af-47e9-b3c9-a60577b19b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Began loading RBSP Data for year: 2012\n",
      "1969573\n",
      "\n",
      "RBSP-A SHAPES BEFORE PREPROCESSING:\n",
      "(3220864,)\n",
      "(3220864,)\n",
      "(3220864,)\n",
      "(3220864,)\n",
      "(3220864,)\n",
      "\n",
      "RBSP-A SHAPES AFTER REMOVING POINTS OUTSIDE BINS and INSIDE PLASMASPHERE:\n",
      "(3220864,)\n",
      "(3220864,)\n",
      "(3220864,)\n",
      "(3220864,)\n",
      "(3220864,)\n",
      "Began loading RBSP Data for year: 2012\n",
      "1919898\n",
      "\n",
      "RBSP-B SHAPES BEFORE PREPROCESSING:\n",
      "(3179153,)\n",
      "(3179153,)\n",
      "(3179153,)\n",
      "(3179153,)\n",
      "(3179153,)\n",
      "\n",
      "RBSP-B SHAPES AFTER REMOVING POINTS OUTSIDE BINS and INSIDE PLASMASPHERE:\n",
      "(3179153,)\n",
      "(3179153,)\n",
      "(3179153,)\n",
      "(3179153,)\n",
      "(3179153,)\n",
      "RBSP Data loaded for year : 2012\n",
      "Began loading POES Data for year : 2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Dev\\Research\\Research_Tools\\ResearchPy\\Lib\\site-packages\\erfa\\core.py:133: ErfaWarning: ERFA function \"dtf2d\" yielded 1 of \"dubious year (Note 6)\"\n",
      "  warn(f'ERFA function \"{func_name}\" yielded {wmsg}', ErfaWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:48\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Dev\\Research\\Research_Tools\\src\\chorus_machine_learning_helper.py:17\u001b[39m, in \u001b[36mload_MPE_year\u001b[39m\u001b[34m(year)\u001b[39m\n\u001b[32m     13\u001b[39m POES = []\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m SAT \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mm01\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mm02\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mm03\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn17\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn18\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn19\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     POES_sat_refs = \u001b[43mdata_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_raw_data_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOES\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSEM\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMPE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43msatellite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m=\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhour\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminute\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtzinfo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUTC\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m=\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m31\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhour\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m23\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminute\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m59\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecond\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m59\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtzinfo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mUTC\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m POES_sat_refs:\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m         \u001b[38;5;66;03m# This was done cause I wanted to scale the MLT before cleaning but Im lazy\u001b[39;00m\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m year < \u001b[32m2014\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Dev\\Research\\Research_Tools\\src\\data_loader.py:395\u001b[39m, in \u001b[36mload_raw_data_from_config\u001b[39m\u001b[34m(id, start, end, satellite, config_path, root_data_dir, use_config_keys_in_subdir, debug, verbose)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths_of_files_within_timeperiod) == \u001b[32m0\u001b[39m:\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_data_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpaths_of_files_within_timeperiod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextension\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariable_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvariables\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Dev\\Research\\Research_Tools\\src\\data_loader.py:274\u001b[39m, in \u001b[36mload_data_files\u001b[39m\u001b[34m(paths, extension, variable_config, debug)\u001b[39m\n\u001b[32m    270\u001b[39m         timestamps_per_file.append(\u001b[38;5;28mlen\u001b[39m(t_array))\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m variable_config[\u001b[33m\"\u001b[39m\u001b[33mtime_dependent\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    273\u001b[39m     unconcatenated_arrays[var].append(\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m         np.ma.MaskedArray.filled(\u001b[43mnc_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m, fill_value=np.nan)\n\u001b[32m    275\u001b[39m     )\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m variable_config[\u001b[33m\"\u001b[39m\u001b[33mfile_dependent\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    278\u001b[39m     unconcatenated_arrays[var].append(\n\u001b[32m    279\u001b[39m         np.expand_dims(\n\u001b[32m    280\u001b[39m             np.ma.MaskedArray.filled(\n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m         )\n\u001b[32m    285\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\netCDF4\\\\_netCDF4.pyx:5122\u001b[39m, in \u001b[36mnetCDF4._netCDF4.Variable.__getitem__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc\\\\netCDF4\\\\_netCDF4.pyx:5270\u001b[39m, in \u001b[36mnetCDF4._netCDF4.Variable._toma\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Dev\\Research\\Research_Tools\\ResearchPy\\Lib\\site-packages\\numpy\\_core\\_methods.py:59\u001b[39m, in \u001b[36m_any\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     56\u001b[39m           initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_prod(a, axis, dtype, out, keepdims, initial, where)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_any\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m, *, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# By default, return a boolean for any and all\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     62\u001b[39m         dtype = bool_dt\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "RBSP = []\n",
    "\n",
    "for SATID in [\"A\", \"B\"]:\n",
    "\n",
    "    # LOAD THE OBSERVED CHORUS\n",
    "    print(f\"Began loading RBSP Data for year: {year}\")\n",
    "    refs = np.load(\n",
    "        file=os.path.join(rbsp_chorus_folder, rf\"observed_chorus_{year}_{SATID}_{MODEL_TYPE}.npz\"),\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    PROBE = {}\n",
    "    PROBE[\"UNIX_TIME\"] = refs[\"UNIX_TIME\"]\n",
    "    PROBE[\"MLT\"] = refs[\"MLT\"]\n",
    "    PROBE[\"MLAT\"] = refs[\"MLAT\"]\n",
    "    PROBE[\"L\"] = refs[\"L\"]\n",
    "    PROBE[\"CHORUS\"] = refs[\"CHORUS\"]\n",
    "\n",
    "    refs.close()\n",
    "\n",
    "    print(f\"\\nRBSP-{SATID} SHAPES BEFORE PREPROCESSING:\")\n",
    "    print(PROBE[\"UNIX_TIME\"].shape)\n",
    "    print(PROBE[\"MLT\"].shape)\n",
    "    print(PROBE[\"L\"].shape)\n",
    "    print(PROBE[\"MLAT\"].shape)\n",
    "    print(PROBE[\"CHORUS\"].shape)\n",
    "\n",
    "    order = np.argsort(PROBE[\"UNIX_TIME\"])\n",
    "    PROBE[\"UNIX_TIME\"] = PROBE[\"UNIX_TIME\"][order]\n",
    "    PROBE[\"MLT\"] = PROBE[\"MLT\"][order]\n",
    "    PROBE[\"L\"] = PROBE[\"L\"][order]\n",
    "    PROBE[\"MLAT\"] = PROBE[\"MLAT\"][order]\n",
    "    PROBE[\"CHORUS\"] = PROBE[\"CHORUS\"][order]\n",
    "\n",
    "    print(f\"\\nRBSP-{SATID} SHAPES AFTER REMOVING POINTS OUTSIDE BINS and INSIDE PLASMASPHERE:\")\n",
    "    print(PROBE[\"UNIX_TIME\"].shape)\n",
    "    print(PROBE[\"MLT\"].shape)\n",
    "    print(PROBE[\"L\"].shape)\n",
    "    print(PROBE[\"MLAT\"].shape)\n",
    "    print(PROBE[\"CHORUS\"].shape)\n",
    "\n",
    "    RBSP.append(PROBE)\n",
    "\n",
    "print(f\"RBSP Data loaded for year : {year}\")\n",
    "print(f\"Began loading POES Data for year : {year}\")\n",
    "\n",
    "POES = chorus_machine_learning_helper.load_MPE_year(year=year)\n",
    "\n",
    "print(f\"Finished loading POES data for year : {year}\")\n",
    "\n",
    "SUPERMAG = chorus_machine_learning_helper.load_SUPERMAG_SME_year(year)\n",
    "\n",
    "print(f\"Loading HP data for year : {year}\")\n",
    "\n",
    "HP_UNIX_TIME, HP = chorus_machine_learning_helper.load_hp30(os.path.join(pdata_folder, \"Hp30_Ap30_1996_2025.txt\"))\n",
    "\n",
    "tstart = datetime.datetime(year=year, month=1, day=1, tzinfo=datetime.UTC).timestamp()\n",
    "tend = datetime.datetime(year=year + 1, month=1, day=1, tzinfo=datetime.UTC).timestamp()\n",
    "\n",
    "order = np.argsort(HP_UNIX_TIME)\n",
    "HP_UNIX_TIME = HP_UNIX_TIME[order]\n",
    "HP = HP[order]\n",
    "\n",
    "time_mask_hp = np.searchsorted(a=HP_UNIX_TIME, v=[tstart, tend])\n",
    "HP_UNIX_TIME = HP_UNIX_TIME[time_mask_hp[0]:time_mask_hp[-1]]\n",
    "HP = HP[time_mask_hp[0]:time_mask_hp[-1]]\n",
    "\n",
    "print(f\"Finished Loading HP data for year : {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954f1ea-6f3c-4a15-812b-fbf4b7a735e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_conditional_total = []\n",
    "X_convolutional_total = []\n",
    "y_total = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for tcurr in np.arange(tstart, tend, T_SIZE):\n",
    "\n",
    "    POES_L = []\n",
    "    POES_MLT = []\n",
    "    POES_FLUX = []\n",
    "\n",
    "    for SAT in POES:\n",
    "        TIME_RANGE = np.searchsorted(SAT[\"UNIX_TIME\"], v=[tcurr, tcurr + T_SIZE])\n",
    "        POES_L.append(SAT[\"L\"][TIME_RANGE[0] : TIME_RANGE[-1]])\n",
    "        POES_MLT.append(SAT[\"MLT\"][TIME_RANGE[0] : TIME_RANGE[-1]])\n",
    "        POES_FLUX.append(SAT[\"BLC_Flux\"][TIME_RANGE[0] : TIME_RANGE[-1], :])\n",
    "\n",
    "    POES_L = np.hstack(POES_L)\n",
    "    POES_MLT = np.hstack(POES_MLT)\n",
    "    POES_FLUX = np.vstack(POES_FLUX)\n",
    "\n",
    "    if POES_L.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    RBSP_L = []\n",
    "    RBSP_MLT = []\n",
    "    RBSP_CHORUS = []\n",
    "    RBSP_MLAT = []\n",
    "\n",
    "    for PROBE in RBSP:\n",
    "\n",
    "        TIME_RANGE = np.searchsorted(\n",
    "            a=PROBE[\"UNIX_TIME\"],\n",
    "            v=[tcurr, tcurr + T_SIZE],\n",
    "        )\n",
    "\n",
    "        RBSP_L.append(PROBE[\"L\"][TIME_RANGE[0] : TIME_RANGE[1]])\n",
    "        RBSP_MLT.append(PROBE[\"MLT\"][TIME_RANGE[0] : TIME_RANGE[1]])\n",
    "        RBSP_CHORUS.append(PROBE[\"CHORUS\"][TIME_RANGE[0] : TIME_RANGE[1]])\n",
    "        RBSP_MLAT.append(PROBE[\"MLAT\"][TIME_RANGE[0] : TIME_RANGE[1]])\n",
    "\n",
    "    RBSP_L = np.hstack(RBSP_L)\n",
    "    RBSP_MLT = np.hstack(RBSP_MLT)\n",
    "    RBSP_CHORUS = np.hstack(RBSP_CHORUS)\n",
    "    RBSP_MLAT = np.hstack(RBSP_MLAT)\n",
    "\n",
    "    if RBSP_L.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    #--------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    POES_theta = (POES_MLT / 24) * (2 * np.pi)\n",
    "\n",
    "    UNDER_130_KeV_FLUX = np.nansum((POES_FLUX[:, :7] * DIFF_E[:7]), axis=1)\n",
    "    TOTAL_FLUX = np.nansum((POES_FLUX[:, :-1] * DIFF_E), axis=1)\n",
    "    FLUX_RATIO = UNDER_130_KeV_FLUX / TOTAL_FLUX\n",
    "\n",
    "    # Query points\n",
    "    r_query = np.random.uniform(low=1, high=10.0, size=SAMPLING_SIZE)\n",
    "    theta_query = np.random.uniform(low=0, high=2*np.pi, size=SAMPLING_SIZE)\n",
    "\n",
    "    ratio_interp = (\n",
    "        dynamic_chorus_model.interpolate_around_points(\n",
    "            POES_L,\n",
    "            POES_theta,\n",
    "            FLUX_RATIO,\n",
    "            r_query,\n",
    "            theta_query,\n",
    "            k=5\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    RBSP_theta = (RBSP_MLT / 24) * (2 * np.pi)\n",
    "\n",
    "    chorus_interp = (\n",
    "        dynamic_chorus_model.interpolate_around_points(\n",
    "            RBSP_L,\n",
    "            RBSP_theta,\n",
    "            RBSP_CHORUS,\n",
    "            r_query,\n",
    "            theta_query,\n",
    "            k=5\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mlat_interp = (\n",
    "        dynamic_chorus_model.interpolate_around_points(\n",
    "            RBSP_L,\n",
    "            RBSP_theta,\n",
    "            RBSP_MLAT,\n",
    "            r_query,\n",
    "            theta_query,\n",
    "            k=5\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    labeled = np.isfinite(chorus_interp) & np.isfinite(ratio_interp)\n",
    "\n",
    "    if not np.any(labeled):\n",
    "        continue\n",
    "\n",
    "    L_sampled = r_query[labeled]\n",
    "    MLT_sampled = (theta_query[labeled] * 24) / (2 * np.pi)\n",
    "    MLAT_sampled = mlat_interp[labeled]\n",
    "    flux_sampled = ratio_interp[labeled]\n",
    "    chorus_sampled = chorus_interp[labeled]\n",
    "\n",
    "    time_mask_hp_2 = np.searchsorted(a=HP_UNIX_TIME, v=[tcurr, tcurr + T_SIZE])\n",
    "    max_hp = np.nanmax(HP[time_mask_hp_2[0] : time_mask_hp_2[-1]])\n",
    "    hp_sampled = np.array([max_hp for l in range(len(L_sampled))])\n",
    "\n",
    "    t_data = np.array([tcurr for l in range(len(L_sampled))])\n",
    "\n",
    "    X_conditional = np.hstack([np.expand_dims(L_sampled, axis=1),\n",
    "                               np.expand_dims(MLT_sampled, axis=1),\n",
    "                               np.expand_dims(MLAT_sampled, axis=1),\n",
    "                               np.expand_dims(flux_sampled, axis=1),\n",
    "                               np.expand_dims(hp_sampled, axis=1),\n",
    "                               np.expand_dims(t_data, axis=1)])\n",
    "\n",
    "    y = chorus_sampled\n",
    "\n",
    "    time_mask_sme = np.searchsorted(a=SUPERMAG[\"UNIX_TIME\"], v=[tcurr - T_SIZE])\n",
    "    X_convolutional = SUPERMAG[\"SME\"][time_mask_sme[0]:time_mask_sme[0] + 512]\n",
    "\n",
    "    if len(X_convolutional) < 512:\n",
    "        continue\n",
    "\n",
    "    X_convolutional = np.vstack([np.expand_dims(X_convolutional, axis=0) for i in range(len(L_sampled))])\n",
    "\n",
    "    X_conditional_total.append(X_conditional)\n",
    "    X_convolutional_total.append(X_convolutional)\n",
    "    y_total.append(y)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Total number of conjunctions: {len(y_total)}\")\n",
    "        print(datetime.datetime.fromtimestamp(tcurr))\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b97f9-0e65-4a1b-ae54-031d46439d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_conditional_total = np.vstack(X_conditional_total)\n",
    "X_convolutional_total = np.vstack(X_convolutional_total)\n",
    "y_total = np.hstack(y_total)\n",
    "\n",
    "print(X_conditional_total.shape)\n",
    "print(X_convolutional_total.shape)\n",
    "print(y_total.shape)\n",
    "\n",
    "np.savez(\n",
    "    file=os.path.join(output_folder, rf\"raw_dataset_{VERSION}_{MODEL_TYPE}_{year}.npz\"),\n",
    "    X_conditional_total=X_conditional_total,\n",
    "    X_convolutional_total=X_convolutional_total,\n",
    "    y_total=y_total\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7af12-7c71-4285-8fc0-885f1ae49548",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(\n",
    "    file=os.path.join(output_folder, rf\"raw_dataset_{VERSION}_{MODEL_TYPE}_{year}.npz\")\n",
    ")\n",
    "\n",
    "print(\"Reading Solar Proton Event List\")\n",
    "\n",
    "SOLAR_PROTON_EVENT_LIST = pd.read_csv(\n",
    "    os.path.join(pdata_folder, r\"SOLAR_PROTON_EVENT_LIST_1976_2024.csv\")\n",
    ")\n",
    "\n",
    "print(\"Finished Reading Solar Proton Event List\")\n",
    "\n",
    "print(\"\\nBefore removing non-valid values\")\n",
    "print(f\"Conditional Shape: {X_conditional_total.shape}\")\n",
    "print(f\"Convolutional Shape: {X_convolutional_total.shape}\")\n",
    "print(f\"Labels Shape: {y_total.shape}\")\n",
    "\n",
    "order_to_sort_conjunctions = np.argsort(\n",
    "    X_conditional_total[:, -1]\n",
    ")  # Sorted based on POES Conjunction time!\n",
    "X_conditional_total = X_conditional_total[order_to_sort_conjunctions, :]\n",
    "X_convolutional_total = X_convolutional_total[order_to_sort_conjunctions, :]\n",
    "y_total = y_total[order_to_sort_conjunctions]\n",
    "\n",
    "all_valid_conditional = np.all(np.isfinite(X_conditional_total), axis=1)\n",
    "all_valid_convolutional = np.all(np.isfinite(X_convolutional_total), axis=1)\n",
    "valid_y = np.isfinite(y_total) & (y_total >= 0)\n",
    "\n",
    "all_valid = all_valid_conditional & all_valid_convolutional & valid_y\n",
    "\n",
    "X_conditional_total = X_conditional_total[all_valid, :]\n",
    "X_convolutional_total = X_convolutional_total[all_valid, :]\n",
    "y_total = y_total[all_valid]\n",
    "\n",
    "print(\"\\nAfter removing non-valid values\")\n",
    "print(f\"Conditional Shape: {X_conditional_total.shape}\")\n",
    "print(f\"Convolutional Shape: {X_convolutional_total.shape}\")\n",
    "print(f\"Labels Shape: {y_total.shape}\")\n",
    "\n",
    "times_sorted = X_conditional_total[:, -1]\n",
    "\n",
    "start_of_sep_events_utc = SOLAR_PROTON_EVENT_LIST[\"START\"]\n",
    "end_of_sep_events_utc = SOLAR_PROTON_EVENT_LIST[\"END\"]\n",
    "zipped_events = list(zip(start_of_sep_events_utc, end_of_sep_events_utc))\n",
    "\n",
    "print(\"Removing high energy solar proton events!\")\n",
    "\n",
    "for SEP in tqdm.tqdm(range(len(zipped_events))):\n",
    "\n",
    "    S = zipped_events[SEP][0].strip()\n",
    "    E = zipped_events[SEP][1].strip()\n",
    "\n",
    "    S_YMDHMS = {\n",
    "        \"year\": int(S[0:4]),\n",
    "        \"month\": int(S[5:7]),\n",
    "        \"day\": int(S[8:10]),\n",
    "        \"hour\": int(S[11:13]),\n",
    "        \"minute\": int(S[13:15]),\n",
    "        \"second\": 0,\n",
    "    }\n",
    "    E_YMDHMS = {\n",
    "        \"year\": int(E[0:4]),\n",
    "        \"month\": int(E[5:7]),\n",
    "        \"day\": int(E[8:10]),\n",
    "        \"hour\": int(E[11:13]),\n",
    "        \"minute\": int(E[13:15]),\n",
    "        \"second\": 0,\n",
    "    }\n",
    "\n",
    "    S_UNIX = astropy.time.Time(S_YMDHMS, format=\"ymdhms\", scale=\"utc\").unix\n",
    "    E_UNIX = astropy.time.Time(E_YMDHMS, format=\"ymdhms\", scale=\"utc\").unix\n",
    "\n",
    "    RANGE_TO_REMOVE = np.searchsorted(a=times_sorted, v=[S_UNIX - T_SIZE, E_UNIX + T_SIZE])\n",
    "\n",
    "    X_conditional_total = np.vstack(\n",
    "        (\n",
    "            X_conditional_total[0 : RANGE_TO_REMOVE[0], :],\n",
    "            X_conditional_total[RANGE_TO_REMOVE[1] :, :],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    X_convolutional_total = np.vstack(\n",
    "        (\n",
    "            X_convolutional_total[0 : RANGE_TO_REMOVE[0], :],\n",
    "            X_convolutional_total[RANGE_TO_REMOVE[1] :, :],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    y_total = np.hstack(\n",
    "        (\n",
    "            y_total[0 : RANGE_TO_REMOVE[0]],\n",
    "            y_total[RANGE_TO_REMOVE[1] :],\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"Finished removing high energy solar proton events!\")\n",
    "\n",
    "print(\"\\nAfter removing solar proton events\")\n",
    "print(f\"Conditional Shape: {X_conditional_total.shape}\")\n",
    "print(f\"Convolutional Shape: {X_convolutional_total.shape}\")\n",
    "print(f\"Labels Shape: {y_total.shape}\")\n",
    "\n",
    "print(\"Saving!\")\n",
    "\n",
    "dataset.close()\n",
    "\n",
    "\n",
    "np.savez(\n",
    "    file=os.path.join(output_folder, rf\"spe_cleaned_dataset_{VERSION}_{MODEL_TYPE}_{year}.npz\"),\n",
    "    X_conditional_total=X_conditional_total,\n",
    "    X_convolutional_total=X_convolutional_total,\n",
    "    y_total=y_total\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11542508-0d4d-46e0-99a5-fd6b9c478dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Min chorus power: {np.min(y_total)}\")\n",
    "print(f\"Max chorus power: {np.max(y_total)}\")\n",
    "\n",
    "print(f\"Min chorus amplitude: {np.min(np.sqrt(y_total))}\")\n",
    "print(f\"Max chorus amplitude: {np.max(np.sqrt(y_total))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82ad370a-d0f0-49a1-b2e2-0377bdc1c73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3393325, 6)\n",
      "(3393325, 512)\n",
      "(3393325,)\n",
      "(3393325, 7)\n",
      "(3393325, 512)\n",
      "(3393325,)\n"
     ]
    }
   ],
   "source": [
    "X_conditional = []\n",
    "X_convolutional = []\n",
    "y = []\n",
    "\n",
    "\n",
    "for year in range(2012, 2020):\n",
    "\n",
    "    d = np.load(\n",
    "        file=os.path.join(output_folder, rf\"spe_cleaned_dataset_{VERSION}_{MODEL_TYPE}_{year}.npz\")\n",
    "    )\n",
    "\n",
    "    X_conditional.append(d[\"X_conditional_total\"])\n",
    "    X_convolutional.append(d[\"X_convolutional_total\"])\n",
    "    y.append(d[\"y_total\"])\n",
    "    d.close()\n",
    "\n",
    "X_conditional = np.vstack(X_conditional)\n",
    "X_convolutional = np.vstack(X_convolutional)\n",
    "y = np.hstack(y)\n",
    "\n",
    "print(X_conditional.shape)\n",
    "print(X_convolutional.shape)\n",
    "print(y.shape)\n",
    "\n",
    "transformed_mlt = np.hstack([np.cos((X_conditional[:, 1:2] / 24) * (2 * np.pi)) , np.sin((X_conditional[:, 1:2] / 24) * (2 * np.pi))])\n",
    "\n",
    "X_conditional = np.hstack([X_conditional[:, 0:1], transformed_mlt, X_conditional[:, 2:]])\n",
    "\n",
    "print(X_conditional.shape)\n",
    "print(X_convolutional.shape)\n",
    "print(y.shape)\n",
    "\n",
    "np.savez(\n",
    "    file=os.path.join(output_folder, rf\"total_dataset_{VERSION}_{MODEL_TYPE}.npz\"),\n",
    "    X_conditional=X_conditional,\n",
    "    X_convolutional=X_convolutional,\n",
    "    y=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424f699-67f4-44db-8944-2fb30a58d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Min chorus power: {np.min(y)}\")\n",
    "print(f\"Max chorus power: {np.max(y)}\")\n",
    "\n",
    "print(f\"Min chorus amplitude: {np.min(np.sqrt(y))}\")\n",
    "print(f\"Max chorus amplitude: {np.max(np.sqrt(y))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c038003-ff98-4dcc-8fa9-477ba557d427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.scatter(X_conditional[:, 0], X_conditional[:, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2119d-8a4f-4857-9db6-0d5e29066d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_conditional[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f91e1d-2d49-4acc-aefa-9e7d0e5dee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tools.plot_2d_heatmap(X_conditional[:, 1],\n",
    "                           X_conditional[:, 0],\n",
    "                           np.sqrt(y),\n",
    "                           bins=20,\n",
    "                           xlim=(-1.0, 1.0),\n",
    "                           ylim=(2, 7),\n",
    "                           xtitle=\"MLT X\",\n",
    "                           ytitle=\"L\",\n",
    "                           ztitle=\"Average Chorus Amplitude (pT)\",\n",
    "                           title=\"Average Chorus Amplitude vs L vs MLT\",\n",
    "                           norm=\"symlog\",\n",
    "                           plot_density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f7b32-dfe3-42fa-b0f2-5fffb2a5a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tools.plot_2d_heatmap(X_conditional[:, 2],\n",
    "                           X_conditional[:, 0],\n",
    "                           np.sqrt(y),\n",
    "                           bins=20,\n",
    "                           xlim=(-1.0, 1.0),\n",
    "                           ylim=(2, 7),\n",
    "                           xtitle=\"MLT Y\",\n",
    "                           ytitle=\"L\",\n",
    "                           ztitle=\"Average Chorus Amplitude (pT)\",\n",
    "                           title=\"Average Chorus Amplitude vs L vs MLT\",\n",
    "                           norm=\"symlog\",\n",
    "                           plot_density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd5b11-faab-4085-8db9-3cebb7033570",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tools.plot_2d_heatmap(X_conditional[:, 0] * X_conditional[:, 1],\n",
    "                           X_conditional[:, 0] * X_conditional[:, 2],\n",
    "                           np.sqrt(y),\n",
    "                           bins=20,\n",
    "                           xlim=(-8.0, 8.0),\n",
    "                           ylim=(-8.0, 8.0),\n",
    "                           xtitle=\"L\",\n",
    "                           ytitle=\"L\",\n",
    "                           ztitle=\"Average Chorus Amplitude (pT)\",\n",
    "                           title=\"Average Chorus Amplitude vs X vs Y (in L/MLT space)\",\n",
    "                           norm=\"symlog\",\n",
    "                           density_norm=\"log\",\n",
    "                           plot_density = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c47c5e-458f-4cf2-bf79-43a5f5f92e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tools.plot_2d_heatmap(X_conditional[:, -3],\n",
    "                           X_conditional[:, -2],\n",
    "                           y,\n",
    "                           bins=20,\n",
    "                           xlim=(0.0, 1.0),\n",
    "                           ylim=(0, 8),\n",
    "                           xtitle=\"Ratio of flux from (30 KeV to 130 KeV) / Total\",\n",
    "                           ytitle=\"Max Kp in past 4 hours\",\n",
    "                           ztitle=\"Average Chorus Power (pT^2)\",\n",
    "                           title=\"Average Chorus Power vs Kp vs Flux Ratio\",\n",
    "                           norm=\"symlog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b904bb-0069-4716-8bca-594f051acbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tools.plot_2d_heatmap(X_conditional[:, -3],\n",
    "                           X_conditional[:, 0],\n",
    "                           y,\n",
    "                           bins=50,\n",
    "                           xlim=(0.0, 1.0),\n",
    "                           ylim=(2, 7),\n",
    "                           xtitle=\"Ratio of flux from (30 KeV to 130 KeV) / Total\",\n",
    "                           ytitle=\"L-Shell\",\n",
    "                           ztitle=\"Average Chorus Power (pT^2)\",\n",
    "                           title=\"Average Chorus Power vs L vs Flux Ratio\",\n",
    "                           norm=\"symlog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72d77c-dd68-4a8a-8295-e7b420379d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tools.plot_2d_heatmap(X_conditional[:, -2],\n",
    "                           X_conditional[:, 0],\n",
    "                           y,\n",
    "                           bins=20,\n",
    "                           xlim=(0.0, 8.0),\n",
    "                           ylim=(2, 7),\n",
    "                           xtitle=\"Hp30 (max in 4h)\",\n",
    "                           ytitle=\"L-Shell\",\n",
    "                           ztitle=\"Average Chorus Power (pT^2)\",\n",
    "                           title=\"Average Chorus Power vs L vs Hp\",\n",
    "                           norm=\"symlog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba9f9c-cfb0-4ee4-839a-4b05de3125d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tools.plot_2d_heatmap(X_conditional[:, -2],\n",
    "                           X_conditional[:, 0],\n",
    "                           np.sqrt(y),\n",
    "                           bins=20,\n",
    "                           xlim=(0.0, 8.0),\n",
    "                           ylim=(2, 7),\n",
    "                           xtitle=\"Hp30 (max in 4h)\",\n",
    "                           ytitle=\"L-Shell\",\n",
    "                           ztitle=\"Average Chorus Amplitude (pT)\",\n",
    "                           title=\"Average Chorus Amplitude vs L vs Hp\",\n",
    "                           norm=matplotlib.colors.SymLogNorm(vmin=0, vmax=1000, linthresh=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f09d5a-d404-4c7f-9c46-772611681531",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=r_query * np.cos(theta_query), y=r_query * np.sin(theta_query), c=ratio_interp, marker=\"+\", s=20, norm=matplotlib.colors.LogNorm(vmin=1e-1, vmax=1e7))\n",
    "plt.scatter(x=POES_L * np.cos(POES_theta), y=POES_L * np.sin(POES_theta), c=FLUX_INTEGRATED, marker=\"x\", s=30, norm=matplotlib.colors.LogNorm(vmin=1e-1, vmax=1e7))\n",
    "plt.colorbar()\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913e9c2-403c-40d7-bd28-5d96e0c01306",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=r_query * np.cos(theta_query), y=r_query * np.sin(theta_query), c=chorus_interp, marker=\"+\", s=20, norm=matplotlib.colors.LogNorm(vmin=1e-1, vmax=1e2))\n",
    "plt.scatter(x=RBSP_L * np.cos(RBSP_theta), y=RBSP_L * np.sin(RBSP_theta), c=RBSP_CHORUS, marker=\"x\", s=30, norm=matplotlib.colors.LogNorm(vmin=1e-1, vmax=1e2))\n",
    "plt.colorbar()\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b01a5-d117-4c20-94c7-5d5bed966aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = np.isfinite(chorus_interp) & np.isfinite(ratio_interp)\n",
    "\n",
    "plt.scatter(x=r_query[labeled] * np.cos(theta_query[labeled]),\n",
    "            y=r_query[labeled] * np.sin(theta_query[labeled]),\n",
    "            c=chorus_interp[labeled],\n",
    "            marker=\"+\",\n",
    "            s=20,\n",
    "            norm=matplotlib.colors.LogNorm(vmin=1e-1, vmax=1e2))\n",
    "\n",
    "plt.xlim(-8, 8)\n",
    "plt.ylim(-8, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493292f2-a278-4845-8a42-23f36ff7137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "def generate_data(n_samples=1000):\n",
    "    x = torch.linspace(-2, 2, n_samples).reshape(-1, 1)\n",
    "    true_mean = x ** 2  # Quadratic mean function\n",
    "    true_variance = 0.1 + 0.5 * x ** 2  # Variance as a function of x\n",
    "    y = true_mean + torch.sqrt(true_variance) * torch.randn(n_samples, 1)\n",
    "    return x, y, true_mean, true_variance\n",
    "\n",
    "# Neural network model predicting mean and log-variance\n",
    "class GaussianRegression(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64):\n",
    "        super(GaussianRegression, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, 1)\n",
    "        self.logvar_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_features = self.shared(x)\n",
    "        mean = self.mean_head(shared_features)\n",
    "        logvar = self.logvar_head(shared_features)\n",
    "        return mean, logvar\n",
    "\n",
    "# Negative log likelihood loss for Gaussian\n",
    "def gaussian_nll_loss(mean, logvar, y):\n",
    "    variance = torch.exp(logvar)\n",
    "    return 0.5 * torch.mean(logvar + (y - mean) ** 2 / variance + np.log(2 * np.pi))\n",
    "\n",
    "# Training function\n",
    "def train_model(model, x, y, epochs=1000, lr=0.01):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        mean, logvar = model(x)\n",
    "        loss = gaussian_nll_loss(mean, logvar, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Generate data\n",
    "x, y, true_mean, true_variance = generate_data()\n",
    "\n",
    "# Initialize model and train\n",
    "model = GaussianRegression()\n",
    "train_model(model, x, y)\n",
    "\n",
    "# Evaluate and plot\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_mean, pred_logvar = model(x)\n",
    "    pred_variance = torch.exp(pred_logvar)\n",
    "\n",
    "# Plotting results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x.numpy(), y.numpy(), alpha=0.2, label='Data')\n",
    "plt.plot(x.numpy(), true_mean.numpy(), 'k-', label='True Mean')\n",
    "plt.plot(x.numpy(), pred_mean.numpy(), 'r--', label='Predicted Mean')\n",
    "plt.fill_between(\n",
    "    x.numpy().flatten(),\n",
    "    (pred_mean - 2 * torch.sqrt(pred_variance)).numpy().flatten(),\n",
    "    (pred_mean + 2 * torch.sqrt(pred_variance)).numpy().flatten(),\n",
    "    color='r', alpha=0.2, label='Predicted ±2σ'\n",
    ")\n",
    "plt.legend()\n",
    "plt.title('Gaussian Regression with Input-Dependent Variance')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2145ad-ac3f-46e2-a433-dddce41a480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64e299-aca8-49b6-ada9-b2709f4aa85d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
