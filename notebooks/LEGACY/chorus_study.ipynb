{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVERYTHING IN HERE IS LEGACY, TURNS OUT HERE WAS A SOLAR PROTON EVENT LIST ON THE NOAA WEBSITE, KEEPING THIS HERE FOR A MOMENT\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# caution: path[0] is reserved for script path (or '' in REPL).\n",
    "sys.path.insert(1, os.path.abspath('./../src'))\n",
    "\n",
    "import data_loader\n",
    "import date_helper\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cdflib.epochs_astropy import CDFAstropy as cdfepoch\n",
    "import pandas as pd\n",
    "import spacepy.time\n",
    "\n",
    "import importlib\n",
    "importlib.reload(data_loader)\n",
    "importlib.reload(date_helper)\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nans_with_previous_valid_data(data):\n",
    "    \n",
    "    is_nan = np.nonzero(np.isnan(data))[0]\n",
    "            \n",
    "    for nan in is_nan:\n",
    "        \n",
    "        i = nan\n",
    "        j = 1\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            if (i - j) == -1:\n",
    "                break\n",
    "            \n",
    "            elif np.isfinite(data[i - j]):\n",
    "                \n",
    "                data[nan] = data[i - j]\n",
    "                break\n",
    "            \n",
    "            j += 1\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contaminated_periods(dates, flux):\n",
    "    \n",
    "    flux = replace_nans_with_previous_valid_data(flux)\n",
    "        \n",
    "    #This removes any NaNs at the beginning that weren't able to be filled with previous values\n",
    "    is_finite = np.isfinite(flux)\n",
    "    dates = dates[is_finite]\n",
    "    flux = flux[is_finite]\n",
    "            \n",
    "    start_pts = []\n",
    "    end_pts = []\n",
    "    \n",
    "    if flux[0] >= 10:\n",
    "        start_pts.append(0)\n",
    "            \n",
    "    a = np.append(np.nan, flux)\n",
    "    b = np.append(flux, np.nan)\n",
    "    \n",
    "    crosses_ten_increasing = np.nonzero((a < 10) & (b >= 10))[0]-1\n",
    "    start_pts.extend(crosses_ten_increasing)\n",
    "    \n",
    "    crosses_ten_decreasing = np.nonzero((a >= 10) & (b < 10))[0]\n",
    "    end_pts.extend(crosses_ten_decreasing)\n",
    "    \n",
    "    if flux[len(flux) - 1] >= 10:\n",
    "        end_pts.append(len(flux) - 1)\n",
    "            \n",
    "    start_dates = dates[start_pts]\n",
    "    end_dates = dates[end_pts]        \n",
    "    \n",
    "    return [(start_dates[i], end_dates[i]) for i in range(len(start_dates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_differential_flux(diff_proton_flux, int_proton_flux_over_500_MeV, diff_proton_lower_energy, diff_proton_upper_energy, diff_proton_effective_energy, timestamps_per_file):\n",
    "    \n",
    "    diff_proton_flux[diff_proton_flux < 0] = np.nan\n",
    "    int_proton_flux_over_500_MeV[int_proton_flux_over_500_MeV < 0] = np.nan\n",
    "    \n",
    "    \n",
    "    integrated_flux = []\n",
    "    num_files = diff_proton_lower_energy.shape[0]\n",
    "    index = 0\n",
    "    for f in range(num_files):\n",
    "        \n",
    "        lower_energies = np.squeeze(diff_proton_lower_energy[f, :, :])\n",
    "        upper_energies = np.squeeze(diff_proton_upper_energy[f, :, :])\n",
    "        effective_energies = np.squeeze(diff_proton_effective_energy[f, :, :])\n",
    "        \n",
    "        sensor_1_lower_energies = lower_energies[0, :]\n",
    "        sensor_2_lower_energies = lower_energies[1, :]\n",
    "        sensor_1_upper_energies = upper_energies[0, :]\n",
    "        sensor_2_upper_energies = upper_energies[1, :]\n",
    "        sensor_1_effective_energies = effective_energies[0, :]\n",
    "        sensor_2_effective_energies = effective_energies[1, :]        \n",
    "        \n",
    "        sensor_1_energies_above_10_MeV = (10.0 <= sensor_1_effective_energies)\n",
    "        sensor_2_energies_above_10_MeV = (10.0 <= sensor_2_effective_energies)\n",
    "                \n",
    "        sensor_1_dE = sensor_1_upper_energies[sensor_1_energies_above_10_MeV] - sensor_1_lower_energies[sensor_1_energies_above_10_MeV]\n",
    "        sensor_2_dE = sensor_2_upper_energies[sensor_2_energies_above_10_MeV] - sensor_2_lower_energies[sensor_2_energies_above_10_MeV]\n",
    "        sensor_1_dE = np.expand_dims(sensor_1_dE, axis=0)\n",
    "        sensor_2_dE = np.expand_dims(sensor_2_dE, axis=0)\n",
    "                        \n",
    "        sensor_1_diff_fluxes = diff_proton_flux[index : (index + timestamps_per_file[f]), 0, :][:, sensor_1_energies_above_10_MeV]\n",
    "        sensor_2_diff_fluxes = diff_proton_flux[index : (index + timestamps_per_file[f]), 1, :][:, sensor_2_energies_above_10_MeV]\n",
    "        \n",
    "        sensor_1_integral_flux_above_10_MeV = np.nansum(sensor_1_diff_fluxes * sensor_1_dE, axis=1) + int_proton_flux_over_500_MeV[index : (index + timestamps_per_file[f]), 0]\n",
    "        sensor_2_integral_flux_above_10_MeV = np.nansum(sensor_2_diff_fluxes * sensor_2_dE, axis=1) + int_proton_flux_over_500_MeV[index : (index + timestamps_per_file[f]), 1]\n",
    "                \n",
    "                \n",
    "        \n",
    "        integrated_flux.extend(np.nanmean([sensor_1_integral_flux_above_10_MeV, sensor_2_integral_flux_above_10_MeV], axis=0))\n",
    "        \n",
    "        index += timestamps_per_file[f]\n",
    "        \n",
    "    \n",
    "    return integrated_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g08\n",
      "g09\n",
      "g10\n",
      "g11\n",
      "g13\n",
      "g14\n",
      "g15\n",
      "g16\n",
      "g17\n",
      "g18\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_24312\\1532985150.py:39: RuntimeWarning: Mean of empty slice\n",
      "  integrated_flux.extend(np.nanmean([sensor_1_integral_flux_above_10_MeV, sensor_2_integral_flux_above_10_MeV], axis=0))\n"
     ]
    }
   ],
   "source": [
    "# Need to load data, clean it, then save it in some sort of numpy array somewhere\n",
    "        \n",
    "periods_to_remove : list[tuple] = [] \n",
    "start_of_study = datetime.datetime(year = 1998, month=1, day=1)\n",
    "end_of_study = datetime.datetime(year = 2024, month=12, day=31, hour=23, minute=59, second=59)\n",
    "\n",
    "#We left g12 out because all of the data in p3_flux_ic is nan!\n",
    "for satellite in [\"g08\", \"g09\", \"g10\", \"g11\", \"g13\", \"g14\", \"g15\", \"g16\", \"g17\", \"g18\"]:\n",
    "    \n",
    "    print(satellite)\n",
    "    \n",
    "    if satellite in [\"g08\", \"g09\", \"g10\", \"g11\"]:\n",
    "        \n",
    "        continue\n",
    "        EPS = data_loader.load_raw_data_from_config(id = [\"GOES\", \"AVG\", \"EPS\"], \n",
    "                                                    start = start_of_study,\n",
    "                                                    end = end_of_study,\n",
    "                                                    satellite = satellite)\n",
    "        \n",
    "        if not EPS:\n",
    "            continue\n",
    "        \n",
    "        time_tag = EPS[\"time_tag\"]\n",
    "        flux = EPS[\"p3_flux_ic\"]\n",
    "        flux[flux < 0] = np.nan\n",
    "        \n",
    "        good_time_tags = (time_tag > 0)\n",
    "        time_tag = time_tag[good_time_tags]\n",
    "        flux = flux[good_time_tags]\n",
    "        \n",
    "        dates = pd.to_datetime(EPS[\"time_tag\"] / 1000, origin=\"unix\", unit=\"s\").to_pydatetime()\n",
    "                \n",
    "        during_study = (start_of_study <= dates) & (dates <= end_of_study)\n",
    "        dates = dates[during_study]\n",
    "        flux = flux[during_study]\n",
    "        \n",
    "        periods_to_remove.extend(find_contaminated_periods(dates, flux))\n",
    "        \n",
    "                    \n",
    "    if satellite in [\"g13\", \"g14\", \"g15\"]:\n",
    "        continue\n",
    "        EPEAD = data_loader.load_raw_data_from_config(id = [\"GOES\", \"AVG\", \"EPEAD\"], \n",
    "                                                        start = start_of_study,\n",
    "                                                        end = end_of_study,\n",
    "                                                        satellite = satellite)\n",
    "        \n",
    "        if not EPEAD:\n",
    "            continue\n",
    "        \n",
    "        time_tag = EPEAD[\"time_tag\"]\n",
    "        flux_east = EPEAD[\"ZPGT10E\"]\n",
    "        flux_west = EPEAD[\"ZPGT10W\"]\n",
    "        flux = (flux_east + flux_west) / 2\n",
    "        flux[flux < 0] = np.nan\n",
    "        \n",
    "        good_time_tags = (time_tag > 0)\n",
    "        time_tag = time_tag[good_time_tags]\n",
    "        flux = flux[good_time_tags]\n",
    "        \n",
    "        dates = pd.to_datetime(EPEAD[\"time_tag\"] / 1000, origin=\"unix\", unit=\"s\").to_pydatetime()\n",
    "        \n",
    "        during_study = (start_of_study <= dates) & (dates <= end_of_study)\n",
    "        dates = dates[during_study]\n",
    "        flux = flux[during_study]\n",
    "        \n",
    "        periods_to_remove.extend(find_contaminated_periods(dates, flux))\n",
    "        \n",
    "        \n",
    "    if satellite in [\"g18\"]:\n",
    "        \n",
    "        SGPS_v1 = data_loader.load_raw_data_from_config(id = [\"GOES\", \"AVG\", \"SGPS\", \"V1\"], \n",
    "                                                        start = start_of_study,\n",
    "                                                        end = end_of_study,\n",
    "                                                        satellite = satellite)\n",
    "        SGPS_v2 = data_loader.load_raw_data_from_config(id = [\"GOES\", \"AVG\", \"SGPS\", \"V2\"], \n",
    "                                                        start = start_of_study,\n",
    "                                                        end = end_of_study,\n",
    "                                                        satellite = satellite)  \n",
    "        SGPS_v3 = data_loader.load_raw_data_from_config(id = [\"GOES\", \"AVG\", \"SGPS\", \"V3\"], \n",
    "                                                        start = start_of_study,\n",
    "                                                        end = end_of_study,\n",
    "                                                        satellite = satellite)\n",
    "        \n",
    "        if SGPS_v1:\n",
    "                                                \n",
    "            time_tag_v1 = SGPS_v1[\"L2_SciData_TimeStamp\"]            \n",
    "            time_tag_v1 = (time_tag_v1 * 1e9).astype(np.int64)\n",
    "            dates_v1 = cdfepoch.to_datetime(time_tag_v1)\n",
    "            \n",
    "            flux_v1 = integrate_differential_flux(diff_proton_flux = SGPS_v1[\"AvgDiffProtonFlux\"] * 1000,\n",
    "                                                  int_proton_flux_over_500_MeV = SGPS_v1[\"AvgIntProtonFlux\"],\n",
    "                                                  diff_proton_lower_energy = SGPS_v1[\"DiffProtonLowerEnergy\"] / 1000.0,\n",
    "                                                  diff_proton_upper_energy = SGPS_v1[\"DiffProtonUpperEnergy\"] / 1000.0,\n",
    "                                                  diff_proton_effective_energy = SGPS_v1[\"DiffProtonEffectiveEnergy\"] / 1000.0,\n",
    "                                                  timestamps_per_file = SGPS_v1[\"timestamps_per_file\"])\n",
    "        if SGPS_v2:\n",
    "            \n",
    "            time_tag_v2 = SGPS_v2[\"L2_SciData_TimeStamp\"]\n",
    "            time_tag_v2 = (time_tag_v2 * 1e9).astype(np.int64)\n",
    "            dates_v2 = cdfepoch.to_datetime(time_tag_v2)\n",
    "            \n",
    "            flux_v2 = integrate_differential_flux(diff_proton_flux = SGPS_v2[\"AvgDiffProtonFlux\"] * 1000,\n",
    "                                                  int_proton_flux_over_500_MeV = SGPS_v2[\"AvgIntProtonFlux\"],\n",
    "                                                  diff_proton_lower_energy = SGPS_v2[\"DiffProtonLowerEnergy\"] / 1000.0,\n",
    "                                                  diff_proton_upper_energy = SGPS_v2[\"DiffProtonUpperEnergy\"] / 1000.0,\n",
    "                                                  diff_proton_effective_energy = SGPS_v2[\"DiffProtonEffectiveEnergy\"] / 1000.0,\n",
    "                                                  timestamps_per_file = SGPS_v2[\"timestamps_per_file\"])\n",
    "            \n",
    "        if SGPS_v3:\n",
    "            \n",
    "            time_tag_v3 = SGPS_v3[\"time\"]            \n",
    "            time_tag_v3 = (time_tag_v3 * 1e9).astype(np.int64)\n",
    "            dates_v3 = cdfepoch.to_datetime(time_tag_v3)\n",
    "            \n",
    "            flux_v3 = integrate_differential_flux(diff_proton_flux = SGPS_v3[\"AvgDiffProtonFlux\"] * 1000,\n",
    "                                                  int_proton_flux_over_500_MeV = SGPS_v3[\"AvgIntProtonFlux\"],\n",
    "                                                  diff_proton_lower_energy = SGPS_v3[\"DiffProtonLowerEnergy\"] / 1000.0,\n",
    "                                                  diff_proton_upper_energy = SGPS_v3[\"DiffProtonUpperEnergy\"] / 1000.0,\n",
    "                                                  diff_proton_effective_energy = SGPS_v3[\"DiffProtonEffectiveEnergy\"] / 1000.0,\n",
    "                                                  timestamps_per_file = SGPS_v3[\"timestamps_per_file\"])\n",
    "            \n",
    "            \n",
    "        '''plt.semilogy(dates, flux)\n",
    "        for start_date in start_dates:\n",
    "            plt.vlines(x=start_date, ymin = 1, ymax = 20000, color=\"green\", linewidth=7)\n",
    "        for end_date in end_dates:\n",
    "            plt.vlines(x=end_date, ymin = 1, ymax = 20000, color=\"red\", linewidth=4)\n",
    "        break'''\n",
    "        \n",
    "        break\n",
    "\n",
    "print(periods_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1470ae8ecf0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.semilogy(dates_v3, flux_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
