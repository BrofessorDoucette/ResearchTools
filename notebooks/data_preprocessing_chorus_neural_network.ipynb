{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# caution: path[0] is reserved for script path (or '' in REPL).\n",
    "sys.path.insert(1, os.path.abspath('./../src'))\n",
    "\n",
    "\n",
    "import data_loader\n",
    "import datetime\n",
    "from cdflib.epochs_astropy import CDFAstropy as cdfepoch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import astropy.time\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import rbsp_chorus_tool\n",
    "\n",
    "import spacepy.irbempy\n",
    "import spacepy.coordinates\n",
    "import spacepy.time\n",
    "import os_helper\n",
    "import tqdm\n",
    "\n",
    "import chorus_machine_learning_helper\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(data_loader)\n",
    "importlib.reload(rbsp_chorus_tool)\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading POES data for year : 1998\n",
      "Loaded the following satellites:\n",
      "[('n15', 775836)]\n"
     ]
    }
   ],
   "source": [
    "#Stage 0\n",
    "\n",
    "def calculate_lstar_for_chunk(time, lat, lon, chunk : tuple):\n",
    "    \n",
    "    ticks = spacepy.time.Ticktock(time[chunk[0] : chunk[-1]], dtype=\"UNX\")\n",
    "    \n",
    "    alt = np.array([807 for i in range(len(time[chunk[0] : chunk[-1]]))])\n",
    "    \n",
    "    coords_data = np.vstack((alt, lat[chunk[0] : chunk[-1]], lon[chunk[0] : chunk[-1]]))\n",
    "    coords = spacepy.coordinates.Coords(data=coords_data.T, carsph=\"sph\", dtype=\"GDZ\", units=[\"km\", \"deg\", \"deg\"])\n",
    "    \n",
    "    return spacepy.irbempy.get_Lstar(ticks = ticks, loci = coords, alpha=0, extMag=\"T89\")[\"Lstar\"].flatten()\n",
    "\n",
    "\n",
    "year = 1998\n",
    "start_of_year_UNIX_TIME = datetime.datetime(year = year, month = 1, day = 1).timestamp()\n",
    "end_of_year_UNIX_TIME = datetime.datetime(year = year + 1, month = 1, day = 1).timestamp()\n",
    "equally_spaced_minutes = np.arange(start = start_of_year_UNIX_TIME, stop = end_of_year_UNIX_TIME + 60, step = 60)\n",
    "\n",
    "MPE = chorus_machine_learning_helper.load_MPE_year(year)\n",
    "\n",
    "print(\"Loaded the following satellites:\")\n",
    "print([(s[\"SATID\"], len(s[\"time\"])) for s in MPE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5000)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 9.699092388153076\n",
      "There were big distances to deal with, probably should check the data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp and xp are not of the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m big_distances[m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     78\u001b[0m     end_index \u001b[38;5;241m=\u001b[39m d\n\u001b[1;32m---> 80\u001b[0m interpolated_between_big_distances \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSAT\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUNIX_TIME\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munix_times_of_averages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstar_calculated\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m non_nan_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misfinite(interpolated_between_big_distances)\n\u001b[0;32m     82\u001b[0m lstar_interpolated[non_nan_values] \u001b[38;5;241m=\u001b[39m interpolated_between_big_distances[non_nan_values]\n",
      "File \u001b[1;32mc:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\lib\\function_base.py:1599\u001b[0m, in \u001b[0;36minterp\u001b[1;34m(x, xp, fp, left, right, period)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     xp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((xp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m-\u001b[39mperiod, xp, xp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mperiod))\n\u001b[0;32m   1597\u001b[0m     fp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((fp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:], fp, fp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m-> 1599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minterp_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: fp and xp are not of the same length."
     ]
    }
   ],
   "source": [
    "data_processed = {}\n",
    "\n",
    "for SAT in MPE:\n",
    "    \n",
    "    unix_times_of_averages = []\n",
    "    avg_geog_lat = []\n",
    "    avg_geog_lon = []\n",
    "    \n",
    "    for MINUTE in equally_spaced_minutes:\n",
    "        \n",
    "        TIME_RANGE = np.searchsorted(a = SAT[\"UNIX_TIME\"], v = [(MINUTE - 30), (MINUTE + 30)])\n",
    "        \n",
    "        if (TIME_RANGE[0] != TIME_RANGE[1]):\n",
    "            \n",
    "            unix_times_of_averages.append(MINUTE)    \n",
    "            avg_geog_lat.append(np.nanmean(SAT[\"geogLat\"][TIME_RANGE[0] : TIME_RANGE[1]]))\n",
    "            y_of_lon = np.nanmean(np.sin(np.deg2rad(SAT[\"geogLon\"][TIME_RANGE[0] : TIME_RANGE[1]])))\n",
    "            x_of_lon = np.nanmean(np.cos(np.deg2rad(SAT[\"geogLon\"][TIME_RANGE[0] : TIME_RANGE[1]])))            \n",
    "            avg_geog_lon.append(np.mod((np.rad2deg(np.arctan2(y_of_lon, x_of_lon)) + 360), 360))\n",
    "        \n",
    "    unix_times_of_averages = np.array(unix_times_of_averages)\n",
    "    avg_geog_lat = np.array(avg_geog_lat)\n",
    "    avg_geog_lon = np.array(avg_geog_lon)\n",
    "    \n",
    "    unix_times_of_averages = unix_times_of_averages[np.isfinite(unix_times_of_averages)]\n",
    "    avg_geog_lat = avg_geog_lat[np.isfinite(avg_geog_lat)]\n",
    "    avg_geog_lon = avg_geog_lon[np.isfinite(avg_geog_lon)]\n",
    "    \n",
    "    big_distances = np.nonzero(((unix_times_of_averages[1:] - unix_times_of_averages[:-1]) > 60))[0] + 1\n",
    "        \n",
    "    queued_work = []\n",
    "    cut_size = 5000\n",
    "    \n",
    "    N = len(unix_times_of_averages)\n",
    "    N_CUTS = N // cut_size\n",
    "    \n",
    "    if N % cut_size != 0:\n",
    "        N_CUTS += 1\n",
    "    \n",
    "    for i in range(N_CUTS):\n",
    "        \n",
    "        if ((i+1) * cut_size) > N:\n",
    "            queued_work.append((i * cut_size, N))\n",
    "        else:\n",
    "            queued_work.append((i * cut_size, (i+1) * cut_size))\n",
    "            \n",
    "        break\n",
    "                        \n",
    "    #--------------------------------------------------\n",
    "\n",
    "    print(queued_work)\n",
    "            \n",
    "    lstar_in_chunks = []\n",
    "    \n",
    "    for work in tqdm.tqdm(queued_work):\n",
    "        \n",
    "        lstar_in_chunks.append(calculate_lstar_for_chunk(unix_times_of_averages, avg_geog_lat, avg_geog_lon, work))\n",
    "            \n",
    "    lstar_calculated = np.abs(np.hstack(lstar_in_chunks))\n",
    "        \n",
    "    lstar_interpolated = np.zeros_like(SAT[\"UNIX_TIME\"])\n",
    "    lstar_interpolated[:] = np.nan\n",
    "    \n",
    "    if len(big_distances) > 0:\n",
    "        \n",
    "        print(\"There were big distances to deal with, probably should check the data!\")\n",
    "                \n",
    "        for m, d in enumerate(big_distances):\n",
    "            \n",
    "            if m == 0 :\n",
    "                \n",
    "                start_index = 0\n",
    "                end_index = d\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                start_index = big_distances[m - 1]\n",
    "                end_index = d\n",
    "                \n",
    "            interpolated_between_big_distances = np.interp(SAT[\"UNIX_TIME\"], unix_times_of_averages[start_index:end_index], lstar_calculated[start_index:end_index], left=np.nan, right=np.nan)\n",
    "            non_nan_values = np.isfinite(interpolated_between_big_distances)\n",
    "            lstar_interpolated[non_nan_values] = interpolated_between_big_distances[non_nan_values]\n",
    "        \n",
    "        #Get the last chunk\n",
    "        \n",
    "        start_index = big_distances[-1]\n",
    "        \n",
    "        interpolated_between_big_distances = np.interp(SAT[\"UNIX_TIME\"], unix_times_of_averages[start_index:], lstar_calculated[start_index:], left=np.nan, right=np.nan)\n",
    "        non_nan_values = np.isfinite(interpolated_between_big_distances)\n",
    "        lstar_interpolated[non_nan_values] = interpolated_between_big_distances[non_nan_values]\n",
    "\n",
    "    else:\n",
    "        \n",
    "        lstar_interpolated = np.interp(SAT[\"UNIX_TIME\"], unix_times_of_averages, lstar_calculated, left=np.nan, right=np.nan)\n",
    "    \n",
    "    print(f\"Finished processing data for : {SAT[\"SATID\"]}\")\n",
    "    \n",
    "    data_processed[SAT[\"SATID\"]] = {\"UNIX_TIME\" : SAT[\"UNIX_TIME\"],\n",
    "                                    \"BLC_Angle\" : SAT[\"BLC_Angle\"],\n",
    "                                    \"BLC_Flux\" : SAT[\"BLC_Flux\"],\n",
    "                                    \"MLT\" : SAT[\"MLT\"],\n",
    "                                    \"Lstar\": lstar_interpolated,\n",
    "                                    \"L\" : SAT[\"lValue\"],\n",
    "                                    \"geogLat\" : SAT[\"geogLat\"],\n",
    "                                    \"geogLon\" : SAT[\"geogLon\"]}\n",
    "\n",
    "output_dir = os.path.abspath(os.path.join(\"./../processed_data_chorus_neural_network/STAGE_0/MPE_DATA_PREPROCESSED_WITH_LSTAR\"))\n",
    "os_helper.verify_output_dir_exists(output_dir, force_creation = True, hint=\"Output directory for L*\")\n",
    "\n",
    "print(f\"Saving data for : {year} to : {output_dir}\")\n",
    "\n",
    "np.savez(file = os.path.abspath(os.path.join(output_dir, f\"MPE_PREPROCESSED_DATA_T89_{year}_Test.npz\")), DATA = data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[         nan -18.23640013 295.16536712]\n",
      " [         nan -14.47854948 294.27519226]\n",
      " [         nan -11.81563314 293.65886471]\n",
      " [         nan  -7.89704967 292.76546478]\n",
      " [         nan  -4.1329748  291.91870117]\n",
      " [         nan  -1.30914995 291.28744507]\n",
      " [         nan   2.92729986 290.34155273]\n",
      " [         nan   6.22227478 289.60281372]\n",
      " [         nan   9.98727489 288.75057984]\n",
      " [         nan  13.28026613 287.99417114]\n",
      " [         nan  16.57139969 287.22281647]\n",
      " [         nan  20.32972527 286.31859589]\n",
      " [         nan  24.08417463 285.38284303]\n",
      " [         nan  27.36526616 284.53241984]\n",
      " [         nan  30.64169979 283.6426697 ]\n",
      " [         nan  34.37982464 282.57037356]\n",
      " [         nan  38.10979843 281.42317967]\n",
      " [         nan  41.36563238 280.34506227]\n",
      " [         nan  44.61224842 279.17397317]\n",
      " [         nan  48.30984783 277.70134749]\n",
      " [         nan  51.99032307 276.04101585]\n",
      " [         nan  55.19343313 274.39277149]\n",
      " [         nan  58.37449741 272.48529885]\n",
      " [         nan  75.27612495 139.41741083]\n",
      " [         nan  72.10187531 131.78897588]]\n"
     ]
    }
   ],
   "source": [
    "points = np.array([(lstar_calculated[i], avg_geog_lat[i], avg_geog_lon[i]) for i in range(len(lstar_calculated))])[0:25]\n",
    "\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lValue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m dt_for_all \u001b[38;5;241m=\u001b[39m [datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mfromtimestamp(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m SAT[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNIX_TIME\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(dt_for_all, lstar_interpolated, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(dt_for_all, \u001b[43mSAT\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lValue'"
     ]
    }
   ],
   "source": [
    "dt_for_all = [datetime.datetime.fromtimestamp(t) for t in SAT[\"UNIX_TIME\"]]\n",
    "\n",
    "plt.plot(dt_for_all, lstar_interpolated, color=\"black\")\n",
    "plt.plot(dt_for_all, SAT[\"lValue\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile './../processed_data_chorus_neural_network/STAGE_0/MPE_DATA_PREPROCESSED_WITH_LSTAR/MPE_PREPROCESSED_DATA_T89_1998.npz' with keys: DATA\n",
      "{'n15': {'UNIX_TIME': array([8.99337638e+08, 8.99337654e+08, 8.99337670e+08, ...,\n",
      "       9.15062372e+08, 9.15062388e+08, 9.15062404e+08]), 'BLC_Angle': array([60.0606569 , 60.22211631, 60.40531253, ..., 49.47817342,\n",
      "       49.62623049, 49.7966486 ]), 'BLC_Flux': array([[1.37677305e+04, 9.73845659e+03, 6.84373393e+03, ...,\n",
      "        2.24349889e+03, 1.50539311e+03, 9.91196794e+02],\n",
      "       [1.54514233e+04, 1.02519394e+04, 6.79301765e+03, ...,\n",
      "        1.95346492e+03, 1.28230903e+03, 8.38451713e+02],\n",
      "       [1.30353254e+04, 8.68605184e+03, 5.78025291e+03, ...,\n",
      "        1.68366880e+03, 1.10983673e+03, 7.28665827e+02],\n",
      "       ...,\n",
      "       [1.18595952e+00, 1.25027509e+00, 1.31661439e+00, ...,\n",
      "        1.51755838e+00, 1.58062968e+00, 1.63832745e+00],\n",
      "       [1.06218402e+00, 1.12195677e+00, 1.18419677e+00, ...,\n",
      "        1.37558442e+00, 1.43558087e+00, 1.48988254e+00],\n",
      "       [1.23398558e+00, 1.27713329e+00, 1.31910974e+00, ...,\n",
      "        1.42517618e+00, 1.44842839e+00, 1.46231748e+00]]), 'MLT': array([19.86599325, 19.8616923 , 19.85752563, ..., 19.03211873,\n",
      "       19.03090007, 19.02234701]), 'Lstar': array([       nan,        nan, 1.20570411, ..., 1.19643105, 1.19709156,\n",
      "              nan]), 'L': array([1.22229993, 1.21739995, 1.21299994, ..., 1.20609999, 1.20920002,\n",
      "       1.21229994]), 'geogLat': array([-19.64830017, -18.70499992, -17.7656002 , ..., -10.29369926,\n",
      "        -9.34939957,  -8.40499973]), 'geogLon': array([295.50500488, 295.27697754, 295.05160522, ..., 294.71780396,\n",
      "       294.50338745, 294.28970337])}}\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\ipykernel\\eventloops.py:145: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  el.exec() if hasattr(el, \"exec\") else el.exec_()\n"
     ]
    }
   ],
   "source": [
    "refs = np.load(fr\"./../processed_data_chorus_neural_network/STAGE_0/MPE_DATA_PREPROCESSED_WITH_LSTAR/MPE_PREPROCESSED_DATA_T89_1998.npz\", allow_pickle=True)\n",
    "\n",
    "print(refs)\n",
    "\n",
    "DATA = refs[\"DATA\"].flatten()[0]\n",
    "print(DATA)\n",
    "SAT = DATA['n15']\n",
    "print(type(SAT))\n",
    "\n",
    "dt_for_all = [datetime.datetime.fromtimestamp(t) for t in SAT[\"UNIX_TIME\"]]\n",
    "\n",
    "plt.plot(dt_for_all, SAT[\"Lstar\"], label=\"L*\", color = \"red\")\n",
    "plt.plot(dt_for_all, SAT[\"L\"], label = \"IGRF Lm\", color = \"black\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interface for stage 1, Designed to do a year at a time\n",
    "\n",
    "year = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1 RBSP Chorus Preprocessing, Obtains clean chorus amplitudes\n",
    "\n",
    "start = datetime.datetime(year = year, month = 1, day = 1)\n",
    "end = datetime.datetime(year = year + 1, month = 1, day = 1)\n",
    "\n",
    "WNA_survey_a = data_loader.load_raw_data_from_config(id=[\"RBSP\", \"EMFISIS\", \"L4\", \"WNA_SURVEY\"],\n",
    "                                                     start=start,\n",
    "                                                     end=end,\n",
    "                                                     satellite=\"a\")\n",
    "\n",
    "WNA_survey_b = data_loader.load_raw_data_from_config(id=[\"RBSP\", \"EMFISIS\", \"L4\", \"WNA_SURVEY\"],\n",
    "                                                     start=start,\n",
    "                                                     end=end,\n",
    "                                                     satellite=\"b\")\n",
    "\n",
    "WFR_spectral_matrix_a = data_loader.load_raw_data_from_config(id=[\"RBSP\", \"EMFISIS\", \"L2\", \"WFR_SPECTRAL_MATRIX_DIAGONAL\"],\n",
    "                                                              start=start,\n",
    "                                                              end=end,\n",
    "                                                              satellite=\"a\")\n",
    "\n",
    "WFR_spectral_matrix_b = data_loader.load_raw_data_from_config(id=[\"RBSP\", \"EMFISIS\", \"L2\", \"WFR_SPECTRAL_MATRIX_DIAGONAL\"],\n",
    "                                                              start=start,\n",
    "                                                              end=end,\n",
    "                                                              satellite=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt_A = WNA_survey_a[\"MLT\"]\n",
    "L_A = WNA_survey_a[\"L\"]\n",
    "epoch_A = WNA_survey_a[\"Epoch\"]\n",
    "\n",
    "time_A = astropy.time.Time(cdfepoch.to_datetime(epoch_A), format=\"datetime\").utc\n",
    "\n",
    "chorus_A = rbsp_chorus_tool.iterate_through_days_and_calculate_chorus_amplitudes(WNA_survey = WNA_survey_a,\n",
    "                                                                                 WFR_spectral_matrix = WFR_spectral_matrix_a)\n",
    "\n",
    "\n",
    "within_epoch_range_A = (start < time_A) & (time_A < end)\n",
    "finite_chorus_A = np.isfinite(chorus_A)\n",
    "#This line might not be necessary but we want to train on clean data, literally any np.nan will fuck it ALL up. Ill probably double check before training\n",
    "all_positive_coordinates_A = (epoch_A > 0) & (mlt_A > 0) & (L_A > 0)\n",
    "\n",
    "epoch_A = epoch_A[within_epoch_range_A & finite_chorus_A & all_positive_coordinates_A]\n",
    "L_A = L_A[within_epoch_range_A & finite_chorus_A & all_positive_coordinates_A]\n",
    "mlt_A = mlt_A[within_epoch_range_A & finite_chorus_A & all_positive_coordinates_A]\n",
    "chorus_A = chorus_A[within_epoch_range_A & finite_chorus_A & all_positive_coordinates_A]\n",
    "\n",
    "\n",
    "mlt_B = WNA_survey_b[\"MLT\"]\n",
    "L_B = WNA_survey_b[\"L\"]\n",
    "epoch_B = WNA_survey_b[\"Epoch\"]\n",
    "time_B = astropy.time.Time(cdfepoch.to_datetime(epoch_B), format=\"datetime\").utc\n",
    "\n",
    "chorus_B = rbsp_chorus_tool.iterate_through_days_and_calculate_chorus_amplitudes(WNA_survey = WNA_survey_b,\n",
    "                                                                                 WFR_spectral_matrix = WFR_spectral_matrix_b)\n",
    "\n",
    "within_epoch_range_B = (start < time_B) & (time_B < end)\n",
    "finite_chorus_B = np.isfinite(chorus_B)\n",
    "all_positive_coordinates_B = (epoch_B > 0) & (mlt_B > 0) & (L_B > 0)\n",
    "\n",
    "epoch_B = epoch_B[within_epoch_range_B & finite_chorus_B & all_positive_coordinates_B]\n",
    "L_B = L_B[within_epoch_range_B & finite_chorus_B & all_positive_coordinates_B]\n",
    "mlt_B = mlt_B[within_epoch_range_B & finite_chorus_B & all_positive_coordinates_B]\n",
    "chorus_B = chorus_B[within_epoch_range_B & finite_chorus_B & all_positive_coordinates_B]\n",
    "\n",
    "print(epoch_A.shape)\n",
    "print(chorus_A.shape)\n",
    "print(L_A.shape)\n",
    "\n",
    "print(epoch_B.shape)\n",
    "print(chorus_B.shape)\n",
    "print(L_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the RBSP stage 1 data, might honestly only need one stage\n",
    "np.savez(file = os.path.abspath(f\"./../processed_data_chorus_neural_network/STAGE_1/RBSP_OBSERVED_CHORUS_{year}.npz\"), \n",
    "         EPOCH_A = epoch_A, \n",
    "         MLT_A = mlt_A, \n",
    "         L_A = L_A, \n",
    "         CHORUS_A = chorus_A,\n",
    "         EPOCH_B = epoch_B, \n",
    "         MLT_B = mlt_B, \n",
    "         L_B = L_B, \n",
    "         CHORUS_B = chorus_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Began processing year : 2012\n",
      "Began loading RBSP Data for year: 2012\n",
      "RBSP Data loaded for year : 2012\n",
      "Began loading POES Data for year : 2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\erfa\\core.py:133: ErfaWarning: ERFA function \"dtf2d\" yielded 1 of \"dubious year (Note 6)\"\n",
      "  warn(f'ERFA function \"{func_name}\" yielded {wmsg}', ErfaWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading POES data for year : 2012\n",
      "Began loading SUPERMAG data for year : 2012\n",
      "Finished loading SUPERMAG data for year : 2012\n",
      "Began loading OMNI data for year : 2012\n",
      "Finished loading OMNI data for year : 2012\n",
      "Finding CONJUNCTIONS for year : 2012\n",
      "Number of records: 1542503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 984718/1542503 [00:03<00:02, 248237.00it/s]C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:214: RuntimeWarning: Mean of empty slice\n",
      "  AVG_AVG_B = np.nanmean(OMNI[\"AVG_B\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:215: RuntimeWarning: Mean of empty slice\n",
      "  AVG_FLOW_SPEED = np.nanmean(OMNI[\"FLOW_SPEED\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:216: RuntimeWarning: Mean of empty slice\n",
      "  AVG_PROTON_DENSITY = np.nanmean(OMNI[\"PROTON_DENSITY\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 1542503/1542503 [01:46<00:00, 14538.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 1658\n",
      "Number of records: 1564594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1564594/1564594 [01:45<00:00, 14777.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 1493\n",
      "Number of records: 1564390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1564390/1564390 [01:44<00:00, 14910.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 1470\n",
      "Number of records: 1578310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1578310/1578310 [01:44<00:00, 15070.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 1483\n",
      "Number of records: 1571588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1571588/1571588 [01:46<00:00, 14787.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 1454\n",
      "Total number of conjunctions so far: 7558\n",
      "Began processing year : 2013\n",
      "Began loading RBSP Data for year: 2013\n",
      "RBSP Data loaded for year : 2013\n",
      "Began loading POES Data for year : 2013\n",
      "Finished loading POES data for year : 2013\n",
      "Began loading SUPERMAG data for year : 2013\n",
      "Finished loading SUPERMAG data for year : 2013\n",
      "Began loading OMNI data for year : 2013\n",
      "Finished loading OMNI data for year : 2013\n",
      "Finding CONJUNCTIONS for year : 2013\n",
      "Number of records: 1409288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6893/1409288 [00:01<04:43, 4950.09it/s]C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:214: RuntimeWarning: Mean of empty slice\n",
      "  AVG_AVG_B = np.nanmean(OMNI[\"AVG_B\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:215: RuntimeWarning: Mean of empty slice\n",
      "  AVG_FLOW_SPEED = np.nanmean(OMNI[\"FLOW_SPEED\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:216: RuntimeWarning: Mean of empty slice\n",
      "  AVG_PROTON_DENSITY = np.nanmean(OMNI[\"PROTON_DENSITY\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 1409288/1409288 [04:26<00:00, 5287.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3560\n",
      "Number of records: 1397910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1397910/1397910 [04:23<00:00, 5300.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3854\n",
      "Number of records: 229970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229970/229970 [00:43<00:00, 5262.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 610\n",
      "Number of records: 1409589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1409589/1409589 [04:25<00:00, 5315.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3986\n",
      "Number of records: 1403042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1403042/1403042 [04:23<00:00, 5323.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3936\n",
      "Total number of conjunctions so far: 23504\n",
      "Began processing year : 2014\n",
      "Began loading RBSP Data for year: 2014\n",
      "RBSP Data loaded for year : 2014\n",
      "Began loading POES Data for year : 2014\n",
      "Finished loading POES data for year : 2014\n",
      "Began loading SUPERMAG data for year : 2014\n",
      "Finished loading SUPERMAG data for year : 2014\n",
      "Began loading OMNI data for year : 2014\n",
      "Finished loading OMNI data for year : 2014\n",
      "Finding CONJUNCTIONS for year : 2014\n",
      "Number of records: 1465525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5757/1465525 [00:01<04:52, 4992.55it/s]C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:214: RuntimeWarning: Mean of empty slice\n",
      "  AVG_AVG_B = np.nanmean(OMNI[\"AVG_B\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:215: RuntimeWarning: Mean of empty slice\n",
      "  AVG_FLOW_SPEED = np.nanmean(OMNI[\"FLOW_SPEED\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:216: RuntimeWarning: Mean of empty slice\n",
      "  AVG_PROTON_DENSITY = np.nanmean(OMNI[\"PROTON_DENSITY\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 1465525/1465525 [04:37<00:00, 5283.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3863\n",
      "Number of records: 1484433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1484433/1484433 [04:41<00:00, 5279.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3976\n",
      "Number of records: 1479400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1479400/1479400 [04:39<00:00, 5292.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3614\n",
      "Number of records: 642159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 642159/642159 [02:00<00:00, 5316.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 1696\n",
      "Number of records: 1496242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1496242/1496242 [04:42<00:00, 5290.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3593\n",
      "Number of records: 1485999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1485999/1485999 [04:41<00:00, 5278.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3634\n",
      "Total number of conjunctions so far: 43880\n",
      "Began processing year : 2015\n",
      "Began loading RBSP Data for year: 2015\n",
      "RBSP Data loaded for year : 2015\n",
      "Began loading POES Data for year : 2015\n",
      "Finished loading POES data for year : 2015\n",
      "Began loading SUPERMAG data for year : 2015\n",
      "Finished loading SUPERMAG data for year : 2015\n",
      "Began loading OMNI data for year : 2015\n",
      "Finished loading OMNI data for year : 2015\n",
      "Finding CONJUNCTIONS for year : 2015\n",
      "Number of records: 1446924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 477/1446924 [00:00<05:06, 4724.17it/s]C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:214: RuntimeWarning: Mean of empty slice\n",
      "  AVG_AVG_B = np.nanmean(OMNI[\"AVG_B\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:215: RuntimeWarning: Mean of empty slice\n",
      "  AVG_FLOW_SPEED = np.nanmean(OMNI[\"FLOW_SPEED\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:216: RuntimeWarning: Mean of empty slice\n",
      "  AVG_PROTON_DENSITY = np.nanmean(OMNI[\"PROTON_DENSITY\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 1446924/1446924 [04:35<00:00, 5257.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3624\n",
      "Number of records: 1447074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1447074/1447074 [04:34<00:00, 5272.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3548\n",
      "Number of records: 1437649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1437649/1437649 [04:32<00:00, 5274.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3803\n",
      "Number of records: 1455435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1455435/1455435 [04:36<00:00, 5262.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3902\n",
      "Number of records: 1454915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1454915/1454915 [04:35<00:00, 5279.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 4116\n",
      "Total number of conjunctions so far: 62873\n",
      "Began processing year : 2016\n",
      "Began loading RBSP Data for year: 2016\n",
      "RBSP Data loaded for year : 2016\n",
      "Began loading POES Data for year : 2016\n",
      "Finished loading POES data for year : 2016\n",
      "Began loading SUPERMAG data for year : 2016\n",
      "Finished loading SUPERMAG data for year : 2016\n",
      "Began loading OMNI data for year : 2016\n",
      "Finished loading OMNI data for year : 2016\n",
      "Finding CONJUNCTIONS for year : 2016\n",
      "Number of records: 1477986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 381/1477986 [00:00<06:28, 3806.76it/s]C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:214: RuntimeWarning: Mean of empty slice\n",
      "  AVG_AVG_B = np.nanmean(OMNI[\"AVG_B\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:215: RuntimeWarning: Mean of empty slice\n",
      "  AVG_FLOW_SPEED = np.nanmean(OMNI[\"FLOW_SPEED\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:216: RuntimeWarning: Mean of empty slice\n",
      "  AVG_PROTON_DENSITY = np.nanmean(OMNI[\"PROTON_DENSITY\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 1477986/1477986 [04:40<00:00, 5269.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 4029\n",
      "Number of records: 1479857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1479857/1479857 [04:40<00:00, 5269.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3738\n",
      "Number of records: 1495294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1495294/1495294 [04:44<00:00, 5254.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 4162\n",
      "Number of records: 1498334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1498334/1498334 [04:45<00:00, 5252.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3820\n",
      "Total number of conjunctions so far: 78622\n",
      "Began processing year : 2017\n",
      "Began loading RBSP Data for year: 2017\n",
      "RBSP Data loaded for year : 2017\n",
      "Began loading POES Data for year : 2017\n",
      "Finished loading POES data for year : 2017\n",
      "Began loading SUPERMAG data for year : 2017\n",
      "Finished loading SUPERMAG data for year : 2017\n",
      "Began loading OMNI data for year : 2017\n",
      "Finished loading OMNI data for year : 2017\n",
      "Finding CONJUNCTIONS for year : 2017\n",
      "Number of records: 1482805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5112/1482805 [00:01<05:00, 4923.90it/s]C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:214: RuntimeWarning: Mean of empty slice\n",
      "  AVG_AVG_B = np.nanmean(OMNI[\"AVG_B\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:215: RuntimeWarning: Mean of empty slice\n",
      "  AVG_FLOW_SPEED = np.nanmean(OMNI[\"FLOW_SPEED\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:216: RuntimeWarning: Mean of empty slice\n",
      "  AVG_PROTON_DENSITY = np.nanmean(OMNI[\"PROTON_DENSITY\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 1482805/1482805 [04:41<00:00, 5264.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3836\n",
      "Number of records: 1481869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1481869/1481869 [04:41<00:00, 5258.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3878\n",
      "Number of records: 1475238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1475238/1475238 [04:41<00:00, 5239.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 4130\n",
      "Number of records: 1489607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1489607/1489607 [04:43<00:00, 5255.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3974\n",
      "Number of records: 1496300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1496300/1496300 [04:48<00:00, 5186.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 4139\n",
      "Total number of conjunctions so far: 98579\n",
      "Began processing year : 2018\n",
      "Began loading RBSP Data for year: 2018\n",
      "RBSP Data loaded for year : 2018\n",
      "Began loading POES Data for year : 2018\n",
      "Finished loading POES data for year : 2018\n",
      "Began loading SUPERMAG data for year : 2018\n",
      "Finished loading SUPERMAG data for year : 2018\n",
      "Began loading OMNI data for year : 2018\n",
      "Finished loading OMNI data for year : 2018\n",
      "Finding CONJUNCTIONS for year : 2018\n",
      "Number of records: 1443579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1443579 [00:00<?, ?it/s]C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:214: RuntimeWarning: Mean of empty slice\n",
      "  AVG_AVG_B = np.nanmean(OMNI[\"AVG_B\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:215: RuntimeWarning: Mean of empty slice\n",
      "  AVG_FLOW_SPEED = np.nanmean(OMNI[\"FLOW_SPEED\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:216: RuntimeWarning: Mean of empty slice\n",
      "  AVG_PROTON_DENSITY = np.nanmean(OMNI[\"PROTON_DENSITY\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 1443579/1443579 [04:37<00:00, 5209.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 4197\n",
      "Number of records: 1472708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1472708/1472708 [04:42<00:00, 5218.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 4284\n",
      "Number of records: 1466108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1466108/1466108 [04:42<00:00, 5182.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 3968\n",
      "Number of records: 1491747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1491747/1491747 [04:45<00:00, 5224.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 4334\n",
      "Number of records: 1490983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1490983/1490983 [04:45<00:00, 5214.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 4450\n",
      "Total number of conjunctions so far: 119812\n",
      "Began processing year : 2019\n",
      "Began loading RBSP Data for year: 2019\n",
      "RBSP Data loaded for year : 2019\n",
      "Began loading POES Data for year : 2019\n",
      "Finished loading POES data for year : 2019\n",
      "Began loading SUPERMAG data for year : 2019\n",
      "Finished loading SUPERMAG data for year : 2019\n",
      "Began loading OMNI data for year : 2019\n",
      "Finished loading OMNI data for year : 2019\n",
      "Finding CONJUNCTIONS for year : 2019\n",
      "Number of records: 1412994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 584/1412994 [00:00<04:20, 5429.07it/s]C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:214: RuntimeWarning: Mean of empty slice\n",
      "  AVG_AVG_B = np.nanmean(OMNI[\"AVG_B\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:215: RuntimeWarning: Mean of empty slice\n",
      "  AVG_FLOW_SPEED = np.nanmean(OMNI[\"FLOW_SPEED\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "C:\\Users\\brofe\\AppData\\Local\\Temp\\ipykernel_16164\\2839501365.py:216: RuntimeWarning: Mean of empty slice\n",
      "  AVG_PROTON_DENSITY = np.nanmean(OMNI[\"PROTON_DENSITY\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Dev\\Research\\REPT_Enhancements_Tool\\ResearchPy\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 1412994/1412994 [02:59<00:00, 7853.86it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 2372\n",
      "Number of records: 1273453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1273453/1273453 [02:55<00:00, 7246.98it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 2295\n",
      "Number of records: 1423905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1423905/1423905 [03:01<00:00, 7830.54it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 2330\n",
      "Number of records: 1450116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1450116/1450116 [03:04<00:00, 7852.97it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 2428\n",
      "Number of records: 1449804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449804/1449804 [03:04<00:00, 7871.10it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conjunctions: 2484\n",
      "Total number of conjunctions so far: 131721\n",
      "Conjunctions to be saved: (131721, 21)\n"
     ]
    }
   ],
   "source": [
    "#Stage 2, clean then combine RBSP, OMNI, and POES Data and find conjunctions between RBSP and POES\n",
    "\n",
    "MAX_L_DIFF = 0.10\n",
    "MAX_MLT_DIFF = 1.5\n",
    "MAX_T_DIFF_SEC = 60\n",
    "\n",
    "L_SCALE = (1.0 / MAX_L_DIFF)**2\n",
    "MLT_SCALE = (1.0 / MAX_MLT_DIFF)**2\n",
    "TIME_SCALE = (1.0 / MAX_T_DIFF_SEC)**2\n",
    "\n",
    "CONJUNCTIONS_TOTAL = []\n",
    "\n",
    "for _year in range(2012, 2020, 1):\n",
    "    \n",
    "    print(f\"Began processing year : {_year}\")\n",
    "    \n",
    "    print(f\"Began loading RBSP Data for year: {_year}\")\n",
    "    refs = np.load(f\"./../processed_data_chorus_neural_network/STAGE_1/RBSP_OBSERVED_CHORUS_{_year}.npz\")\n",
    "    RBSP_A = {}\n",
    "    RBSP_A[\"EPOCH\"] = refs[\"EPOCH_A\"]\n",
    "    RBSP_A[\"MLT\"] = refs[\"MLT_A\"]\n",
    "    RBSP_A[\"L\"] = refs[\"L_A\"]\n",
    "    RBSP_A[\"CHORUS\"] = refs[\"CHORUS_A\"]\n",
    "    \n",
    "    RBSP_B = {}\n",
    "    RBSP_B[\"EPOCH\"] = refs[\"EPOCH_B\"]\n",
    "    RBSP_B[\"MLT\"] = refs[\"MLT_B\"]\n",
    "    RBSP_B[\"L\"] = refs[\"L_B\"]\n",
    "    RBSP_B[\"CHORUS\"] = refs[\"CHORUS_B\"]\n",
    "    \n",
    "    refs.close()\n",
    "    \n",
    "    RBSP_A[\"UNIX_TIME\"] = cdfepoch.unixtime(RBSP_A[\"EPOCH\"])\n",
    "    RBSP_B[\"UNIX_TIME\"] = cdfepoch.unixtime(RBSP_B[\"EPOCH\"])\n",
    "    \n",
    "    order_A = np.argsort(RBSP_A[\"UNIX_TIME\"])\n",
    "    order_B = np.argsort(RBSP_B[\"UNIX_TIME\"])\n",
    "    \n",
    "    RBSP_A[\"UNIX_TIME\"] = RBSP_A[\"UNIX_TIME\"][order_A]\n",
    "    RBSP_A[\"EPOCH\"] = RBSP_A[\"EPOCH\"][order_A]\n",
    "    RBSP_A[\"MLT\"] = RBSP_A[\"MLT\"][order_A]\n",
    "    RBSP_A[\"L\"] = RBSP_A[\"L\"][order_A]\n",
    "    RBSP_A[\"CHORUS\"] = RBSP_A[\"CHORUS\"][order_A]\n",
    "\n",
    "    RBSP_B[\"UNIX_TIME\"] = RBSP_B[\"UNIX_TIME\"][order_B]\n",
    "    RBSP_B[\"EPOCH\"] = RBSP_B[\"EPOCH\"][order_B]\n",
    "    RBSP_B[\"MLT\"] = RBSP_B[\"MLT\"][order_B]\n",
    "    RBSP_B[\"L\"] = RBSP_B[\"L\"][order_B]\n",
    "    RBSP_B[\"CHORUS\"] = RBSP_B[\"CHORUS\"][order_B]\n",
    "    \n",
    "    RBSP = [RBSP_A, RBSP_B]\n",
    "    print(f\"RBSP Data loaded for year : {_year}\")\n",
    "    \n",
    "    print(f\"Began loading POES Data for year : {_year}\")\n",
    "    POES = []\n",
    "    \n",
    "    for SAT in [\"m01\", \"m02\", \"m03\", \"n15\", \"n16\", \"n17\", \"n18\", \"n19\"]:\n",
    "        \n",
    "        POES_sat_refs = data_loader.load_raw_data_from_config(id=[\"POES\", \"SEM\", \"MPE\"],\n",
    "                                                              satellite=SAT,\n",
    "                                                              start=datetime.datetime(year=_year, month=1, day=1),\n",
    "                                                              end=datetime.datetime(year=_year, month=12, day=31, hour=23, minute=59, second=59))\n",
    "        \n",
    "        if POES_sat_refs:\n",
    "            \n",
    "            \n",
    "            #This was done cause I wanted to scale the MLT before cleaning but Im lazy\n",
    "            if _year < 2014:\n",
    "                POES_sat_refs[\"MLT\"] = (POES_sat_refs[\"MLT\"] / 360.0) * 24.0\n",
    "            \n",
    "            valid_times = np.isfinite(POES_sat_refs[\"time\"]) & (0 < POES_sat_refs[\"time\"])\n",
    "            valid_BLC_Angle = np.isfinite(POES_sat_refs[\"BLC_Angle\"]) & (0 < POES_sat_refs[\"BLC_Angle\"])\n",
    "            valid_BLC_Flux = np.all(np.isfinite(POES_sat_refs[\"BLC_Flux\"][:, :8]), axis=1) & np.all((0 < POES_sat_refs[\"BLC_Flux\"][:, :8]), axis=1)\n",
    "            valid_MLT = np.isfinite(POES_sat_refs[\"MLT\"]) & (0 < POES_sat_refs[\"MLT\"]) & (POES_sat_refs[\"MLT\"] < 24)\n",
    "            valid_L = np.isfinite(POES_sat_refs[\"lValue\"]) & (0 < POES_sat_refs[\"lValue\"]) & (POES_sat_refs[\"lValue\"] < 10)\n",
    "            valid_points = valid_times & valid_BLC_Angle & valid_BLC_Flux & valid_MLT & valid_L\n",
    "            \n",
    "            if np.any(valid_points):\n",
    "                \n",
    "                POES_sat_refs[\"time\"] = POES_sat_refs[\"time\"][valid_points]\n",
    "                POES_sat_refs[\"BLC_Angle\"] = POES_sat_refs[\"BLC_Angle\"][valid_points]\n",
    "                POES_sat_refs[\"BLC_Flux\"] = POES_sat_refs[\"BLC_Flux\"][valid_points, :8]\n",
    "                POES_sat_refs[\"MLT\"] = POES_sat_refs[\"MLT\"][valid_points]\n",
    "                POES_sat_refs[\"lValue\"] = POES_sat_refs[\"lValue\"][valid_points]\n",
    "            \n",
    "                if _year < 2014:\n",
    "                                    \n",
    "                    POES_sat_refs[\"UNIX_TIME\"] = cdfepoch.unixtime(POES_sat_refs[\"time\"])\n",
    "                else:\n",
    "                    POES_sat_refs[\"UNIX_TIME\"] = (POES_sat_refs[\"time\"] / 1000)\n",
    "                    \n",
    "                #Sort them so assumptions for binary search are satisfied:\n",
    "                order = np.argsort(POES_sat_refs[\"UNIX_TIME\"])\n",
    "                POES_sat_refs[\"time\"] = POES_sat_refs[\"time\"][order]\n",
    "                POES_sat_refs[\"UNIX_TIME\"] = POES_sat_refs[\"UNIX_TIME\"][order]\n",
    "                POES_sat_refs[\"BLC_Angle\"] = POES_sat_refs[\"BLC_Angle\"][order]\n",
    "                POES_sat_refs[\"BLC_Flux\"] = POES_sat_refs[\"BLC_Flux\"][order, :]\n",
    "                POES_sat_refs[\"MLT\"] = POES_sat_refs[\"MLT\"][order]\n",
    "                POES_sat_refs[\"lValue\"] = POES_sat_refs[\"lValue\"][order]\n",
    "                \n",
    "                POES.append(POES_sat_refs)\n",
    "    \n",
    "    if not POES:\n",
    "        print(f\"No POES satellite coverage found for year : {_year}\")\n",
    "        print(f\"SKIPPING YEAR : {_year}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Finished loading POES data for year : {_year}\")\n",
    "    \n",
    "    print(f\"Began loading SUPERMAG data for year : {_year}\")\n",
    "    SUPERMAG_df = pd.read_csv(f\"./../processed_data_chorus_neural_network/SUPERMAG_SME/sme_{_year}.csv\")\n",
    "    SUPERMAG = {}\n",
    "    \n",
    "    valid_SME = np.isfinite(SUPERMAG_df[\"SME\"]) & (0 < SUPERMAG_df[\"SME\"])\n",
    "    \n",
    "    if not np.any(valid_SME):\n",
    "        print(f\"No valid SME for year : {_year}\")\n",
    "        print(f\"SKIPPING YEAR : {_year}\")\n",
    "        continue\n",
    "    \n",
    "    SUPERMAG[\"SME\"] = np.array(SUPERMAG_df[\"SME\"][valid_SME])\n",
    "    SUPERMAG[\"Date_UTC\"] = np.array(SUPERMAG_df[\"Date_UTC\"][valid_SME])\n",
    "    SUPERMAG[\"UNIX_TIME\"] = astropy.time.Time(SUPERMAG[\"Date_UTC\"].astype(str), scale=\"utc\", in_subfmt='date_hms').unix\n",
    "    \n",
    "    order = np.argsort(SUPERMAG[\"UNIX_TIME\"])\n",
    "    SUPERMAG[\"SME\"] = SUPERMAG[\"SME\"][order]\n",
    "    SUPERMAG[\"Date_UTC\"] = SUPERMAG[\"Date_UTC\"][order]\n",
    "    SUPERMAG[\"UNIX_TIME\"] = SUPERMAG[\"UNIX_TIME\"][order]\n",
    "    \n",
    "    print(f\"Finished loading SUPERMAG data for year : {_year}\")\n",
    "    \n",
    "    print(f\"Began loading OMNI data for year : {_year}\")\n",
    "    OMNI_refs = data_loader.load_raw_data_from_config(id = [\"OMNI\", \"ONE_MIN_RESOLUTION\"], \n",
    "                                                 start = datetime.datetime(year=_year, month=1, day=1),\n",
    "                                                 end = datetime.datetime(year=_year, month=12, day=31, hour=23, minute=59, second=59))\n",
    "    OMNI = {}\n",
    "    \n",
    "    valid_times = np.isfinite(OMNI_refs[\"Epoch\"]) & (0 < OMNI_refs[\"Epoch\"])\n",
    "    valid_AVG_B = np.isfinite(OMNI_refs[\"F\"]) & (0 <= OMNI_refs[\"F\"]) & (OMNI_refs[\"F\"] < 9990)\n",
    "    valid_FLOW_SPEED = np.isfinite(OMNI_refs[\"flow_speed\"]) & (0 <= OMNI_refs[\"flow_speed\"]) & (OMNI_refs[\"flow_speed\"] < 99900)\n",
    "    valid_PROTON_DENSITY = np.isfinite(OMNI_refs[\"proton_density\"]) & (-900 <= OMNI_refs[\"proton_density\"]) & (OMNI_refs[\"proton_density\"] < 900)\n",
    "    valid_SYM_H = np.isfinite(OMNI_refs[\"SYM_H\"]) & (-99000 <= OMNI_refs[\"SYM_H\"]) & (OMNI_refs[\"SYM_H\"] < 99900)\n",
    "    valid_points = valid_times & valid_AVG_B & valid_FLOW_SPEED & valid_PROTON_DENSITY & valid_SYM_H\n",
    "    \n",
    "    if(not np.any(valid_points)):\n",
    "        print(f\"No valid OMNI DATA for year : {_year}\")\n",
    "        print(f\"SKIPPING YEAR : {_year}\")\n",
    "        continue\n",
    "    \n",
    "    OMNI[\"EPOCH\"] = OMNI_refs[\"Epoch\"][valid_points]\n",
    "    OMNI[\"UNIX_TIME\"] = cdfepoch.unixtime(OMNI_refs[\"Epoch\"][valid_points])\n",
    "    OMNI[\"AVG_B\"] = OMNI_refs[\"F\"][valid_points]\n",
    "    OMNI[\"FLOW_SPEED\"] = OMNI_refs[\"flow_speed\"][valid_points]\n",
    "    OMNI[\"PROTON_DENSITY\"] = OMNI_refs[\"proton_density\"][valid_points]\n",
    "    OMNI[\"SYM_H\"] = OMNI_refs[\"SYM_H\"][valid_points]\n",
    "    \n",
    "    order = np.argsort(OMNI[\"UNIX_TIME\"])\n",
    "    OMNI[\"EPOCH\"] = OMNI[\"EPOCH\"][order]\n",
    "    OMNI[\"UNIX_TIME\"] = OMNI[\"UNIX_TIME\"][order]\n",
    "    OMNI[\"AVG_B\"] = OMNI[\"AVG_B\"][order]\n",
    "    OMNI[\"FLOW_SPEED\"] = OMNI[\"FLOW_SPEED\"][order]\n",
    "    OMNI[\"PROTON_DENSITY\"] = OMNI[\"PROTON_DENSITY\"][order]\n",
    "    OMNI[\"SYM_H\"] = OMNI[\"SYM_H\"][order]\n",
    "    \n",
    "    print(f\"Finished loading OMNI data for year : {_year}\")\n",
    "    \n",
    "    print(f\"Finding CONJUNCTIONS for year : {_year}\")\n",
    "    CONJUNCTIONS_YEAR = []\n",
    "    for POES_SAT in POES:\n",
    "        \n",
    "        NUMBER_OF_RECORDS = len(POES_SAT[\"UNIX_TIME\"])\n",
    "        CONJUNCTIONS = []\n",
    "        \n",
    "        print(f\"Number of records: {NUMBER_OF_RECORDS}\")\n",
    "                \n",
    "        for T in tqdm.tqdm(range(NUMBER_OF_RECORDS)):\n",
    "            \n",
    "            UNIX_TIME = POES_SAT[\"UNIX_TIME\"][T]\n",
    "            L = POES_SAT[\"lValue\"][T]\n",
    "            MLT = POES_SAT[\"MLT\"][T]\n",
    "            FLUX_SPECTRUM = POES_SAT[\"BLC_Flux\"][T, :]\n",
    "                        \n",
    "            for RBSP_PROBE in RBSP:\n",
    "                \n",
    "                TIME_RANGE = np.searchsorted(a = RBSP_PROBE[\"UNIX_TIME\"], v = [(UNIX_TIME - MAX_T_DIFF_SEC), (UNIX_TIME + MAX_T_DIFF_SEC)])\n",
    "\n",
    "                NUM_CANDIDATES = 0\n",
    "                TOTAL_TIME = 0\n",
    "                TOTAL_L = 0\n",
    "                TOTAL_DEL_MLT = 0\n",
    "                TOTAL_CHORUS = 0\n",
    "                \n",
    "                for POINT in range(TIME_RANGE[0], TIME_RANGE[1], 1):\n",
    "                    \n",
    "                    del_L = (L - RBSP_PROBE[\"L\"][POINT])\n",
    "                    del_MLT = np.min( [(max(MLT, RBSP_PROBE[\"MLT\"][POINT]) -  min(MLT, RBSP_PROBE[\"MLT\"][POINT])),\n",
    "                                      ((24 - max(MLT, RBSP_PROBE[\"MLT\"][POINT])) + (min(MLT, RBSP_PROBE[\"MLT\"][POINT]) - 0))])\n",
    "                    \n",
    "                    if (del_L**2 < MAX_L_DIFF**2) and (del_MLT**2 < MAX_MLT_DIFF**2):\n",
    "                                                \n",
    "                        NUM_CANDIDATES += 1\n",
    "                        TOTAL_TIME += RBSP_PROBE[\"UNIX_TIME\"][POINT]\n",
    "                        TOTAL_L += RBSP_PROBE[\"L\"][POINT]\n",
    "                        TOTAL_DEL_MLT += del_MLT\n",
    "                        TOTAL_CHORUS += RBSP_PROBE[\"CHORUS\"][POINT]\n",
    "                        \n",
    "                if NUM_CANDIDATES == 0:\n",
    "                    continue\n",
    "                \n",
    "                TIME_RANGE = np.searchsorted(a = SUPERMAG[\"UNIX_TIME\"], v = [(UNIX_TIME - MAX_T_DIFF_SEC), (UNIX_TIME + MAX_T_DIFF_SEC)])\n",
    "                AVG_SME = np.nanmean(SUPERMAG[\"SME\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
    "\n",
    "                TIME_RANGE = np.searchsorted(a = OMNI[\"UNIX_TIME\"], v = [(UNIX_TIME - MAX_T_DIFF_SEC), (UNIX_TIME + MAX_T_DIFF_SEC)])\n",
    "                AVG_AVG_B = np.nanmean(OMNI[\"AVG_B\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
    "                AVG_FLOW_SPEED = np.nanmean(OMNI[\"FLOW_SPEED\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
    "                AVG_PROTON_DENSITY = np.nanmean(OMNI[\"PROTON_DENSITY\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
    "                AVG_SYM_H = np.nanmean(OMNI[\"SYM_H\"][TIME_RANGE[0]:TIME_RANGE[1]])\n",
    "                \n",
    "                if np.isfinite(AVG_SME) & np.isfinite(AVG_AVG_B) & np.isfinite(AVG_FLOW_SPEED) & np.isfinite(AVG_PROTON_DENSITY) & np.isfinite(AVG_SYM_H):\n",
    "                    \n",
    "                    \n",
    "                    CONJUNCTION =  [UNIX_TIME, \n",
    "                                    L, \n",
    "                                    MLT,\n",
    "                                    *FLUX_SPECTRUM,\n",
    "                                    TOTAL_TIME / NUM_CANDIDATES, #TIME\n",
    "                                    TOTAL_L / NUM_CANDIDATES, #L\n",
    "                                    0,\n",
    "                                    TOTAL_DEL_MLT / NUM_CANDIDATES, #del_MLT\n",
    "                                    TOTAL_CHORUS / NUM_CANDIDATES, #CHORUS\n",
    "                                    AVG_SME, \n",
    "                                    AVG_AVG_B,\n",
    "                                    AVG_FLOW_SPEED, \n",
    "                                    AVG_PROTON_DENSITY,\n",
    "                                    AVG_SYM_H]\n",
    "                                                \n",
    "                    CONJUNCTIONS.append(CONJUNCTION)\n",
    "        \n",
    "\n",
    "        print(f\"Number of conjunctions: {len(CONJUNCTIONS)}\")\n",
    "        \n",
    "        CONJUNCTIONS_YEAR.extend(CONJUNCTIONS)\n",
    "    \n",
    "    \n",
    "    CONJUNCTIONS_TOTAL.extend(CONJUNCTIONS_YEAR)\n",
    "    \n",
    "    print(f\"Total number of conjunctions so far: {len(CONJUNCTIONS_TOTAL)}\")\n",
    "\n",
    "CONJUNCTIONS_TO_BE_SAVED = np.vstack(CONJUNCTIONS_TOTAL)\n",
    "\n",
    "print(f\"Conjunctions to be saved: {CONJUNCTIONS_TO_BE_SAVED.shape}\")\n",
    "\n",
    "np.savez(f\"./../processed_data_chorus_neural_network/STAGE_2/v5a/CONJUNCTIONS_v5a.npz\",\n",
    "        CONJUNCTIONS=CONJUNCTIONS_TO_BE_SAVED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 3, Look at the data and make sure its good enough, then remove solar proton events\n",
    "version = \"v5a\"\n",
    "\n",
    "CONJUNCTIONS_REFS = np.load(f\"./../processed_data_chorus_neural_network/STAGE_2/{version}/CONJUNCTIONS_{version}.npz\")\n",
    "\n",
    "CONJUNCTIONS_TESTING = CONJUNCTIONS_REFS[\"CONJUNCTIONS\"]\n",
    "\n",
    "CONJUNCTIONS_REFS.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''PERCENTAGES_COVERED = [25.7899, \n",
    "                       25.7872,\n",
    "                       16.9213,\n",
    "                       16.8872,\n",
    "                       23.2865,\n",
    "                       23.2174,\n",
    "                       21.2469,\n",
    "                       21.1556,\n",
    "                       17.7844,\n",
    "                       17.9721,\n",
    "                       22.3004,\n",
    "                       22.5147,\n",
    "                       23.2432,\n",
    "                       23.2491,\n",
    "                       20.0671,\n",
    "                       19.0985] these were for v2a'''\n",
    "\n",
    "'''CONJUNCTION =  [UNIX_TIME, \n",
    "                    L, \n",
    "                    MLT,\n",
    "                    *FLUX_SPECTRUM,\n",
    "                    candidate[0], #TIME\n",
    "                    candidate[1], #L\n",
    "                    candidate[2], #MLT\n",
    "                    candidate[3], #del_MLT\n",
    "                    candidate[4], #CHORUS\n",
    "                    AVG_SME, \n",
    "                    AVG_AVG_B,\n",
    "                    AVG_FLOW_SPEED, \n",
    "                    AVG_PROTON_DENSITY,\n",
    "                    AVG_SYM_H]'''\n",
    "\n",
    "CONJUNCTIONS_POES_TIME = CONJUNCTIONS_TESTING[:, 0]\n",
    "CONJUNCTIONS_POES_L = CONJUNCTIONS_TESTING[:, 1]\n",
    "CONJUNCTIONS_POES_MLT = CONJUNCTIONS_TESTING[:, 2]\n",
    "CONJUNCTIONS_POES_FLUX = CONJUNCTIONS_TESTING[:, 3:-10]\n",
    "CONJUNCTIONS_RBSP_TIME = CONJUNCTIONS_TESTING[:, -10]\n",
    "CONJUNCTIONS_RBSP_L = CONJUNCTIONS_TESTING[:, -9]\n",
    "CONJUNCTIONS_RBSP_MLT = CONJUNCTIONS_TESTING[:, -8]\n",
    "CONJUNCTIONS_RBSP_DEL_MLT = CONJUNCTIONS_TESTING[:, -7]\n",
    "CONJUNCTIONS_RBSP_CHORUS = CONJUNCTIONS_TESTING[:, -6]\n",
    "CONJUNCTIONS_AVG_SME = CONJUNCTIONS_TESTING[:, -5]\n",
    "CONJUNCTIONS_AVG_AVG_B = CONJUNCTIONS_TESTING[:, -4]\n",
    "CONJUNCTIONS_AVG_FLOW_SPEED = CONJUNCTIONS_TESTING[:, -3]\n",
    "CONJUNCTIONS_AVG_PROTON_DENSITY = CONJUNCTIONS_TESTING[:, -2]\n",
    "CONJUNCTIONS_AVG_SYM_H = CONJUNCTIONS_TESTING[:, -1]\n",
    "\n",
    "with open(f\"./../processed_data_chorus_neural_network/STAGE_2/{version}/CONJUNCTIONS_{version}.txt\", \"w\") as f:\n",
    "    f.write(\"\\nConjunctions:\\n\")\n",
    "    f.write(f\"Number of conjunctions: {CONJUNCTIONS_TESTING.shape[0]} [#]\\n\")\n",
    "    f.write(f\"Minimum RBSP Time: {np.min(CONJUNCTIONS_RBSP_TIME)} [seconds since unix epoch]\\n\")\n",
    "    f.write(f\"Maximum RBSP Time: {np.max(CONJUNCTIONS_RBSP_TIME)} [seconds since unix epoch]\\n\")\n",
    "    f.write(f\"Minimum POES Time: {np.min(CONJUNCTIONS_POES_TIME)} [seconds since unix epoch]\\n\")\n",
    "    f.write(f\"Maximum POES Time: {np.max(CONJUNCTIONS_POES_TIME)} [seconds since unix epoch]\\n\")\n",
    "    \n",
    "    f.write(f\"\\nL:\\n\")\n",
    "    f.write(f\"Mean Difference: {np.mean(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L)} [L]\\n\")\n",
    "    f.write(f\"Standard deviation of Difference {np.std(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L)} [L]\\n\")\n",
    "    f.write(f\"Minimum Absolute Difference : {np.min(np.abs(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L))} [L]\\n\")\n",
    "    f.write(f\"Maximum Absolute Difference : {np.max(np.abs(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L))} [L]\\n\")\n",
    "\n",
    "    f.write(\"\\nMLT: \\n\")\n",
    "    f.write(f\"Mean Absolute Difference: {np.mean(CONJUNCTIONS_RBSP_DEL_MLT)} [MLT]\\n\")\n",
    "    f.write(f\"Standard deviation of Absolute Difference {np.std(CONJUNCTIONS_RBSP_DEL_MLT)} [MLT]\\n\")\n",
    "    f.write(f\"Minimum Absolute Difference : {np.min(np.abs(CONJUNCTIONS_RBSP_DEL_MLT))} [MLT]\\n\")\n",
    "    f.write(f\"Maximum Absolute Difference : {np.max(np.abs(CONJUNCTIONS_RBSP_DEL_MLT))} [MLT]\\n\")\n",
    "\n",
    "    f.write(f\"\\nTime: \\n\")\n",
    "    f.write(f\"Mean Difference: {np.mean(CONJUNCTIONS_POES_TIME - CONJUNCTIONS_RBSP_TIME)} [s]\\n\")\n",
    "    f.write(f\"Standard deviation of Difference {np.std(CONJUNCTIONS_POES_TIME - CONJUNCTIONS_RBSP_TIME)} [s]\\n\")\n",
    "    f.write(f\"Minimum Absolute Difference : {np.min(np.abs(CONJUNCTIONS_POES_TIME - CONJUNCTIONS_RBSP_TIME))} [s]\\n\")\n",
    "    f.write(f\"Maximum Absolute Difference : {np.max(np.abs(CONJUNCTIONS_POES_TIME - CONJUNCTIONS_RBSP_TIME))} [s]\\n\")\n",
    "\n",
    "    f.write(f\"\\nChorus: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_RBSP_CHORUS)} [pT]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_RBSP_CHORUS)} [pT]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_RBSP_CHORUS)} [pT]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_RBSP_CHORUS)} [pT]\\n\")\n",
    "\n",
    "\n",
    "    f.write(f\"\\nSME: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_SME)} [nT]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_SME)} [nT]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_SME)} [nT]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_SME)} [nT]\\n\")\n",
    "\n",
    "    f.write(f\"\\nAVG_B: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_AVG_B)} [nT]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_AVG_B)} [nT]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_AVG_B)} [nT]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_AVG_B)} [nT]\\n\")\n",
    "\n",
    "    f.write(f\"\\nFlow Speed: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_FLOW_SPEED)} [km/s]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_FLOW_SPEED)} [km/s]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_FLOW_SPEED)} [km/s]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_FLOW_SPEED)} [km/s]\\n\")\n",
    "    \n",
    "    f.write(f\"\\nProton Density: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_PROTON_DENSITY)} [n/cc]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_PROTON_DENSITY)} [n/cc]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_PROTON_DENSITY)} [n/cc]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_PROTON_DENSITY)} [n/cc]\\n\")\n",
    "    \n",
    "    f.write(f\"\\nSYM_H: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_SYM_H)} [nT]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_SYM_H)} [nT]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_SYM_H)} [nT]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_SYM_H)} [nT]\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"RBSP - Closest POES L Shell Comparison\")\n",
    "plt.xlabel(\"RBSP L-Shell\")\n",
    "plt.ylabel(\"Closest POES L-Shell\")\n",
    "plt.hlines(y = 4, xmin=1, xmax=7, color=\"black\")\n",
    "plt.vlines(x = 4, ymin=1, ymax=7, color=\"black\")\n",
    "\n",
    "plt.scatter(CONJUNCTIONS_RBSP_L, CONJUNCTIONS_POES_L)\n",
    "\n",
    "print(f\"Mean difference: {np.mean(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L)}\")\n",
    "print(f\"Standard deviation of difference {np.std(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L)}\")\n",
    "print(f\"Maximum difference : {np.max(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"RBSP MLT - Closest POES MLT\")\n",
    "plt.xlabel(\"RBSP MLT\")\n",
    "plt.ylabel(\"Closest POES MLT\")\n",
    "plt.hlines(y = 12, xmin=0, xmax=25, color=\"black\")\n",
    "plt.vlines(x = 12, ymin=0, ymax=25, color=\"black\")\n",
    "\n",
    "plt.scatter(CONJUNCTIONS_RBSP_MLT, CONJUNCTIONS_POES_MLT)\n",
    "\n",
    "print(f\"Mean difference: {np.mean(CONJUNCTIONS_RBSP_DEL_MLT)}\")\n",
    "print(f\"Standard deviation of difference {np.std(CONJUNCTIONS_RBSP_DEL_MLT)}\")\n",
    "print(f\"Maximum difference : {np.max(CONJUNCTIONS_RBSP_DEL_MLT)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 3 Continued, Removing solar proton events!\n",
    "\n",
    "version = \"v5a\"\n",
    "\n",
    "CONJUNCTIONS_REFS = np.load(f\"./../processed_data_chorus_neural_network/STAGE_2/{version}/CONJUNCTIONS_{version}.npz\")\n",
    "\n",
    "CONJUNCTIONS = CONJUNCTIONS_REFS[\"CONJUNCTIONS\"]\n",
    "\n",
    "CONJUNCTIONS_REFS.close()\n",
    "\n",
    "SOLAR_PROTON_EVENT_LIST = pd.read_csv(f\"./../processed_data_chorus_neural_network/SOLAR_PROTON_EVENT_LIST_1976_2024.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting shape of conjunctions list: (131721, 21)\n",
      "Removing high energy solar proton events!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:01<00:00, 207.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished removing high energy solar proton events!\n",
      "Saving!\n",
      "Creating documentation of dataset!\n",
      "Finished!\n",
      "Ending shape of conjunctions : (130741, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''CONJUNCTION =  [UNIX_TIME, \n",
    "                    L, \n",
    "                    MLT,\n",
    "                    *FLUX_SPECTRUM,\n",
    "                    candidate[0], #TIME\n",
    "                    candidate[1], #L\n",
    "                    candidate[2], #MLT\n",
    "                    candidate[3], #del_MLT\n",
    "                    candidate[4], #CHORUS\n",
    "                    AVG_SME, \n",
    "                    AVG_AVG_B,\n",
    "                    AVG_FLOW_SPEED, \n",
    "                    AVG_PROTON_DENSITY,\n",
    "                    AVG_SYM_H]'''\n",
    "\n",
    "\n",
    "order_to_sort_conjunctions = np.argsort(CONJUNCTIONS[:, 0]) #Sorted based on POES Conjunction time!\n",
    "SORTED_CONJUNCTIONS = CONJUNCTIONS[order_to_sort_conjunctions, :]\n",
    "\n",
    "print(f\"Starting shape of conjunctions list: {SORTED_CONJUNCTIONS.shape}\")\n",
    "\n",
    "SORTED_POES_CONJUNCTION_TIMES = SORTED_CONJUNCTIONS[:, 0]\n",
    "\n",
    "START_OF_SEP_EVENTS_UTC = SOLAR_PROTON_EVENT_LIST[\"START\"]\n",
    "END_OF_SEP_EVENTS_UTC = SOLAR_PROTON_EVENT_LIST[\"END\"]\n",
    "ZIPPED_EVENTS = list(zip(START_OF_SEP_EVENTS_UTC, END_OF_SEP_EVENTS_UTC))\n",
    "\n",
    "print(f\"Removing high energy solar proton events!\")\n",
    "\n",
    "for SEP_EVENT in tqdm.tqdm(range(len(ZIPPED_EVENTS))):\n",
    "    \n",
    "    START = ZIPPED_EVENTS[SEP_EVENT][0].strip()\n",
    "    END = ZIPPED_EVENTS[SEP_EVENT][1].strip()\n",
    "    \n",
    "    START_YMDHMS = {'year': int(START[0:4]), 'month': int(START[5:7]), 'day': int(START[8:10]), 'hour': int(START[11:13]), 'minute': int(START[13:15]), 'second': 0}\n",
    "    END_YMDHMS = {'year': int(END[0:4]), 'month': int(END[5:7]), 'day': int(END[8:10]), 'hour': int(END[11:13]), 'minute': int(END[13:15]), 'second': 0}\n",
    "    \n",
    "    START_UNIX = astropy.time.Time(START_YMDHMS, format=\"ymdhms\", scale='utc').unix\n",
    "    END_UNIX = astropy.time.Time(END_YMDHMS, format=\"ymdhms\", scale='utc').unix\n",
    "\n",
    "    RANGE_TO_REMOVE = np.searchsorted(a = SORTED_POES_CONJUNCTION_TIMES, v = [START_UNIX, END_UNIX])\n",
    "    \n",
    "    SORTED_CONJUNCTIONS = np.vstack((SORTED_CONJUNCTIONS[0:RANGE_TO_REMOVE[0], :], SORTED_CONJUNCTIONS[RANGE_TO_REMOVE[1]:, :]))\n",
    "\n",
    "print(f\"Finished removing high energy solar proton events!\")\n",
    "\n",
    "print(f\"Saving!\")\n",
    "\n",
    "CLEANED_CONJUNCTIONS = SORTED_CONJUNCTIONS #Should be cleaned by now!\n",
    "\n",
    "np.savez(f\"./../processed_data_chorus_neural_network/STAGE_3/{version}/CLEANED_CONJUNCTIONS_{version}.npz\",\n",
    "        CONJUNCTIONS=CLEANED_CONJUNCTIONS)\n",
    "\n",
    "CONJUNCTIONS_POES_TIME = CLEANED_CONJUNCTIONS[:, 0]\n",
    "CONJUNCTIONS_POES_L = CLEANED_CONJUNCTIONS[:, 1]\n",
    "CONJUNCTIONS_POES_MLT = CLEANED_CONJUNCTIONS[:, 2]\n",
    "CONJUNCTIONS_POES_FLUX = CLEANED_CONJUNCTIONS[:, 3:-10]\n",
    "CONJUNCTIONS_RBSP_TIME = CLEANED_CONJUNCTIONS[:, -10]\n",
    "CONJUNCTIONS_RBSP_L = CLEANED_CONJUNCTIONS[:, -9]\n",
    "CONJUNCTIONS_RBSP_MLT = CLEANED_CONJUNCTIONS[:, -8]\n",
    "CONJUNCTIONS_RBSP_DEL_MLT = CLEANED_CONJUNCTIONS[:, -7]\n",
    "CONJUNCTIONS_RBSP_CHORUS = CLEANED_CONJUNCTIONS[:, -6]\n",
    "CONJUNCTIONS_AVG_SME = CLEANED_CONJUNCTIONS[:, -5]\n",
    "CONJUNCTIONS_AVG_AVG_B = CLEANED_CONJUNCTIONS[:, -4]\n",
    "CONJUNCTIONS_AVG_FLOW_SPEED = CLEANED_CONJUNCTIONS[:, -3]\n",
    "CONJUNCTIONS_AVG_PROTON_DENSITY = CLEANED_CONJUNCTIONS[:, -2]\n",
    "CONJUNCTIONS_AVG_SYM_H = CLEANED_CONJUNCTIONS[:, -1]\n",
    "\n",
    "print(f\"Creating documentation of dataset!\")\n",
    "\n",
    "\n",
    "with open(f\"./../processed_data_chorus_neural_network/STAGE_3/{version}/CLEANED_CONJUNCTIONS_{version}.txt\", \"w\") as f:\n",
    "    f.write(\"\\nConjunctions:\\n\")\n",
    "    f.write(f\"Number of conjunctions: {CLEANED_CONJUNCTIONS.shape[0]} [#]\\n\")\n",
    "    f.write(f\"Number lost from cleaning solar proton events: {CONJUNCTIONS.shape[0] - CLEANED_CONJUNCTIONS.shape[0]} [#]\\n\")\n",
    "    f.write(f\"Minimum RBSP Time: {np.min(CONJUNCTIONS_RBSP_TIME)} [seconds since unix epoch]\\n\")\n",
    "    f.write(f\"Maximum RBSP Time: {np.max(CONJUNCTIONS_RBSP_TIME)} [seconds since unix epoch]\\n\")\n",
    "    f.write(f\"Minimum POES Time: {np.min(CONJUNCTIONS_POES_TIME)} [seconds since unix epoch]\\n\")\n",
    "    f.write(f\"Maximum POES Time: {np.max(CONJUNCTIONS_POES_TIME)} [seconds since unix epoch]\\n\")\n",
    "    \n",
    "    f.write(f\"\\nL:\\n\")\n",
    "    f.write(f\"Mean Difference: {np.mean(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L)} [L]\\n\")\n",
    "    f.write(f\"Standard deviation of Difference {np.std(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L)} [L]\\n\")\n",
    "    f.write(f\"Minimum Absolute Difference : {np.min(np.abs(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L))} [L]\\n\")\n",
    "    f.write(f\"Maximum Absolute Difference : {np.max(np.abs(CONJUNCTIONS_POES_L - CONJUNCTIONS_RBSP_L))} [L]\\n\")\n",
    "\n",
    "    f.write(\"\\nMLT: \\n\")\n",
    "    f.write(f\"Mean Absolute Difference: {np.mean(CONJUNCTIONS_RBSP_DEL_MLT)} [MLT]\\n\")\n",
    "    f.write(f\"Standard deviation of Absolute Difference {np.std(CONJUNCTIONS_RBSP_DEL_MLT)} [MLT]\\n\")\n",
    "    f.write(f\"Minimum Absolute Difference : {np.min(np.abs(CONJUNCTIONS_RBSP_DEL_MLT))} [MLT]\\n\")\n",
    "    f.write(f\"Maximum Absolute Difference : {np.max(np.abs(CONJUNCTIONS_RBSP_DEL_MLT))} [MLT]\\n\")\n",
    "\n",
    "    f.write(f\"\\nTime: \\n\")\n",
    "    f.write(f\"Mean Difference: {np.mean(CONJUNCTIONS_POES_TIME - CONJUNCTIONS_RBSP_TIME)} [s]\\n\")\n",
    "    f.write(f\"Standard deviation of Difference {np.std(CONJUNCTIONS_POES_TIME - CONJUNCTIONS_RBSP_TIME)} [s]\\n\")\n",
    "    f.write(f\"Minimum Absolute Difference : {np.min(np.abs(CONJUNCTIONS_POES_TIME - CONJUNCTIONS_RBSP_TIME))} [s]\\n\")\n",
    "    f.write(f\"Maximum Absolute Difference : {np.max(np.abs(CONJUNCTIONS_POES_TIME - CONJUNCTIONS_RBSP_TIME))} [s]\\n\")\n",
    "\n",
    "    f.write(f\"\\nChorus: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_RBSP_CHORUS)} [pT]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_RBSP_CHORUS)} [pT]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_RBSP_CHORUS)} [pT]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_RBSP_CHORUS)} [pT]\\n\")\n",
    "\n",
    "\n",
    "    f.write(f\"\\nSME: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_SME)} [nT]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_SME)} [nT]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_SME)} [nT]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_SME)} [nT]\\n\")\n",
    "\n",
    "    f.write(f\"\\nAVG_B: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_AVG_B)} [nT]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_AVG_B)} [nT]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_AVG_B)} [nT]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_AVG_B)} [nT]\\n\")\n",
    "\n",
    "    f.write(f\"\\nFlow Speed: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_FLOW_SPEED)} [km/s]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_FLOW_SPEED)} [km/s]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_FLOW_SPEED)} [km/s]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_FLOW_SPEED)} [km/s]\\n\")\n",
    "    \n",
    "    f.write(f\"\\nProton Density: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_PROTON_DENSITY)} [n/cc]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_PROTON_DENSITY)} [n/cc]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_PROTON_DENSITY)} [n/cc]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_PROTON_DENSITY)} [n/cc]\\n\")\n",
    "    \n",
    "    f.write(f\"\\nSYM_H: \\n\")\n",
    "    f.write(f\"Mean: {np.mean(CONJUNCTIONS_AVG_SYM_H)} [nT]\\n\")\n",
    "    f.write(f\"Standard Deviation: {np.std(CONJUNCTIONS_AVG_SYM_H)} [nT]\\n\")\n",
    "    f.write(f\"Minimum: {np.min(CONJUNCTIONS_AVG_SYM_H)} [nT]\\n\")\n",
    "    f.write(f\"Maximum: {np.max(CONJUNCTIONS_AVG_SYM_H)} [nT]\\n\")\n",
    "\n",
    "print(f\"Finished!\")\n",
    "print(f\"Ending shape of conjunctions : {CLEANED_CONJUNCTIONS.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 4, Create datasets used for training, testing, etc\n",
    "\n",
    "version = \"v5a\"\n",
    "\n",
    "CONJUNCTIONS_REFS = np.load(f\"./../processed_data_chorus_neural_network/STAGE_3/{version}/CLEANED_CONJUNCTIONS_{version}.npz\")\n",
    "\n",
    "CONJUNCTIONS = CONJUNCTIONS_REFS[\"CONJUNCTIONS\"]\n",
    "\n",
    "CONJUNCTIONS_REFS.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130741, 21)\n",
      "Number of conjunctions between feb1 and apr1 2013: 3284\n",
      "Number of conjunctions with non-zero chorus: 130740\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 6)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "(127456, 1)\n",
      "Mean L: 4.0540999945598255, STD L: 1.316954157213637\n",
      "Mean fluxes : (1, 6)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 6)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 1)\n",
      "(3284, 1)\n"
     ]
    }
   ],
   "source": [
    "print(CONJUNCTIONS.shape)\n",
    "\n",
    "CONJUNCTIONS_CHORUS = CONJUNCTIONS[:, -6]\n",
    "\n",
    "CONJUNCTIONS_RBSP_TIME = CONJUNCTIONS[:, -10]\n",
    "\n",
    "where_chorus_greater_zero = (0 < CONJUNCTIONS_CHORUS)\n",
    "\n",
    "jan1_unix = astropy.time.Time({\"year\":2016, \"month\":1, \"day\":1, \"hour\":0, \"minute\":0, \"second\":0}, format=\"ymdhms\", scale=\"utc\").unix\n",
    "apr1_unix = astropy.time.Time({\"year\":2016, \"month\":4, \"day\":1, \"hour\":0, \"minute\":0, \"second\":0}, format=\"ymdhms\", scale=\"utc\").unix\n",
    "\n",
    "where_between_feb1_apr1_2013 = (jan1_unix < CONJUNCTIONS_RBSP_TIME) & (CONJUNCTIONS_RBSP_TIME < apr1_unix)\n",
    "\n",
    "train_test_subset_selected = where_chorus_greater_zero & ~where_between_feb1_apr1_2013\n",
    "validation_subset_selected = where_chorus_greater_zero & where_between_feb1_apr1_2013\n",
    "\n",
    "print(f\"Number of conjunctions between feb1 and apr1 2013: {np.count_nonzero(where_between_feb1_apr1_2013)}\")\n",
    "print(f\"Number of conjunctions with non-zero chorus: {np.count_nonzero(where_chorus_greater_zero)}\")\n",
    "\n",
    "CONJUNCTIONS_POES_TIME = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, 0], axis = 1)\n",
    "CONJUNCTIONS_POES_L = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, 1], axis = 1)\n",
    "CONJUNCTIONS_POES_MLT = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, 2], axis = 1)\n",
    "CONJUNCTIONS_POES_FLUX = CONJUNCTIONS[train_test_subset_selected, 3:9]\n",
    "CONJUNCTIONS_RBSP_TIME = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -10], axis = 1)\n",
    "CONJUNCTIONS_RBSP_L = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -9], axis = 1)\n",
    "CONJUNCTIONS_RBSP_MLT = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -8], axis = 1)\n",
    "CONJUNCTIONS_RBSP_DEL_MLT = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -7], axis = 1)\n",
    "CONJUNCTIONS_RBSP_CHORUS = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -6], axis = 1)\n",
    "CONJUNCTIONS_AVG_SME = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -5], axis = 1)\n",
    "CONJUNCTIONS_AVG_AVG_B = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -4], axis = 1)\n",
    "CONJUNCTIONS_AVG_FLOW_SPEED = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -3], axis = 1)\n",
    "CONJUNCTIONS_AVG_PROTON_DENSITY = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -2], axis = 1)\n",
    "CONJUNCTIONS_AVG_SYM_H = np.expand_dims(CONJUNCTIONS[train_test_subset_selected, -1], axis = 1)\n",
    "\n",
    "print(CONJUNCTIONS_RBSP_TIME.shape)\n",
    "print(CONJUNCTIONS_RBSP_L.shape)\n",
    "print(CONJUNCTIONS_RBSP_MLT.shape)\n",
    "print(CONJUNCTIONS_RBSP_CHORUS.shape)\n",
    "print(CONJUNCTIONS_POES_TIME.shape)\n",
    "print(CONJUNCTIONS_POES_L.shape)\n",
    "print(CONJUNCTIONS_POES_MLT.shape)\n",
    "print(CONJUNCTIONS_RBSP_DEL_MLT.shape)\n",
    "print(CONJUNCTIONS_POES_FLUX.shape)\n",
    "print(CONJUNCTIONS_AVG_SME.shape)\n",
    "print(CONJUNCTIONS_AVG_AVG_B.shape)\n",
    "print(CONJUNCTIONS_AVG_FLOW_SPEED.shape)\n",
    "print(CONJUNCTIONS_AVG_PROTON_DENSITY.shape)\n",
    "print(CONJUNCTIONS_AVG_SYM_H.shape)\n",
    "\n",
    "mean_L = np.nanmean(CONJUNCTIONS_POES_L)\n",
    "std_L = np.std(CONJUNCTIONS_POES_L)\n",
    "\n",
    "print(f\"Mean L: {mean_L}, STD L: {std_L}\")\n",
    "\n",
    "mean_fluxes = np.expand_dims(np.nanmean(np.log(CONJUNCTIONS_POES_FLUX), axis = 0), axis=0)\n",
    "std_fluxes = np.expand_dims(np.nanstd(np.log(CONJUNCTIONS_POES_FLUX), axis = 0), axis = 0)\n",
    "\n",
    "print(f\"Mean fluxes : {mean_fluxes.shape}\")\n",
    "\n",
    "mean_sme = np.nanmean(CONJUNCTIONS_AVG_SME)\n",
    "std_sme = np.std(CONJUNCTIONS_AVG_SME)\n",
    "\n",
    "mean_avg_b = np.nanmean(CONJUNCTIONS_AVG_AVG_B)\n",
    "std_avg_b = np.std(CONJUNCTIONS_AVG_AVG_B)\n",
    "\n",
    "mean_flow_speed = np.nanmean(CONJUNCTIONS_AVG_FLOW_SPEED)\n",
    "std_flow_speed = np.std(CONJUNCTIONS_AVG_FLOW_SPEED)\n",
    "\n",
    "mean_avg_proton_density = np.nanmean(CONJUNCTIONS_AVG_PROTON_DENSITY)\n",
    "std_avg_proton_density = np.std(CONJUNCTIONS_AVG_PROTON_DENSITY)\n",
    "\n",
    "mean_avg_sym_h = np.nanmean(CONJUNCTIONS_AVG_SYM_H)\n",
    "std_avg_sym_h = np.std(CONJUNCTIONS_AVG_SYM_H)\n",
    "\n",
    "FEATURES = np.hstack(((CONJUNCTIONS_POES_L - mean_L) / std_L, \n",
    "                      np.sin((CONJUNCTIONS_POES_MLT * 2 * np.pi) / 24.0), \n",
    "                      np.cos((CONJUNCTIONS_POES_MLT * 2 * np.pi) / 24.0),\n",
    "                      ((np.log(CONJUNCTIONS_POES_FLUX) - mean_fluxes) / std_fluxes)[:, [0, -1]],\n",
    "                      #(CONJUNCTIONS_AVG_SME - mean_sme)  / std_sme))\n",
    "                      #(CONJUNCTIONS_AVG_AVG_B - mean_avg_b) / std_avg_b,\n",
    "                      (CONJUNCTIONS_AVG_FLOW_SPEED - mean_flow_speed) / std_flow_speed))\n",
    "                      #(CONJUNCTIONS_AVG_PROTON_DENSITY - mean_avg_proton_density) / std_avg_proton_density,\n",
    "                      #(CONJUNCTIONS_AVG_SYM_H - mean_avg_sym_h) / std_avg_sym_h))\n",
    "                      \n",
    "#SMALL VALIDATION SET:\n",
    "CONJUNCTIONS_POES_TIME_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, 0], axis = 1)\n",
    "CONJUNCTIONS_POES_L_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, 1], axis = 1)\n",
    "CONJUNCTIONS_POES_MLT_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, 2], axis = 1)\n",
    "CONJUNCTIONS_POES_FLUX_VALIDATION = CONJUNCTIONS[validation_subset_selected, 3:9]\n",
    "CONJUNCTIONS_RBSP_TIME_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -10], axis = 1)\n",
    "CONJUNCTIONS_RBSP_L_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -9], axis = 1)\n",
    "CONJUNCTIONS_RBSP_MLT_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -8], axis = 1)\n",
    "CONJUNCTIONS_RBSP_DEL_MLT_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -7], axis = 1)\n",
    "CONJUNCTIONS_RBSP_CHORUS_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -6], axis = 1)\n",
    "CONJUNCTIONS_AVG_SME_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -5], axis = 1)\n",
    "CONJUNCTIONS_AVG_AVG_B_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -4], axis = 1)\n",
    "CONJUNCTIONS_AVG_FLOW_SPEED_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -3], axis = 1)\n",
    "CONJUNCTIONS_AVG_PROTON_DENSITY_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -2], axis = 1)\n",
    "CONJUNCTIONS_AVG_SYM_H_VALIDATION = np.expand_dims(CONJUNCTIONS[validation_subset_selected, -1], axis = 1)\n",
    "\n",
    "print(CONJUNCTIONS_RBSP_TIME_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_RBSP_L_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_RBSP_MLT_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_RBSP_CHORUS_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_POES_TIME_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_POES_L_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_POES_MLT_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_RBSP_DEL_MLT_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_POES_FLUX_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_AVG_SME_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_AVG_AVG_B_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_AVG_FLOW_SPEED_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_AVG_PROTON_DENSITY_VALIDATION.shape)\n",
    "print(CONJUNCTIONS_AVG_SYM_H_VALIDATION.shape)\n",
    "\n",
    "\n",
    "VALIDATION_FEATURES = np.hstack(((CONJUNCTIONS_POES_L_VALIDATION - mean_L) / std_L, \n",
    "                                np.sin((CONJUNCTIONS_POES_MLT_VALIDATION * 2 * np.pi) / 24.0), \n",
    "                                np.cos((CONJUNCTIONS_POES_MLT_VALIDATION * 2 * np.pi) / 24.0),\n",
    "                                ((np.log(CONJUNCTIONS_POES_FLUX_VALIDATION) - mean_fluxes) / std_fluxes)[:, [0, -1]],\n",
    "                                #(CONJUNCTIONS_AVG_SME_VALIDATION - mean_sme) / std_sme))\n",
    "                                #(CONJUNCTIONS_AVG_AVG_B_VALIDATION - mean_avg_b) / std_avg_b,\n",
    "                                (CONJUNCTIONS_AVG_FLOW_SPEED_VALIDATION - mean_flow_speed) / std_flow_speed))\n",
    "                                #(CONJUNCTIONS_AVG_PROTON_DENSITY_VALIDATION - mean_avg_proton_density) / std_avg_proton_density,\n",
    "                                #(CONJUNCTIONS_AVG_SYM_H_VALIDATION - mean_avg_sym_h) / std_avg_sym_h))\n",
    "        \n",
    "np.savez(f\"./../processed_data_chorus_neural_network/STAGE_4/{version}/MODEL_READY_DATA_{version}.npz\",\n",
    "        FEATURES = FEATURES,\n",
    "        LABELS = CONJUNCTIONS_RBSP_CHORUS,\n",
    "        VALIDATION_FEATURES = VALIDATION_FEATURES,\n",
    "        VALIDATION_LABELS = CONJUNCTIONS_RBSP_CHORUS_VALIDATION,\n",
    "        TRAINING_MLT = CONJUNCTIONS_POES_MLT,\n",
    "        MEAN_FLUXES = mean_fluxes,\n",
    "        STD_FLUXES = std_fluxes,\n",
    "        MEAN_SME = mean_sme,\n",
    "        STD_SME = std_sme,\n",
    "        MEAN_AVG_B = mean_avg_b,\n",
    "        STD_AVG_B = std_avg_b,\n",
    "        MEAN_FLOW_SPEED = mean_flow_speed,\n",
    "        STD_FLOW_SPEED = std_flow_speed,\n",
    "        MEAN_AVG_PROTON_DENSITY = mean_avg_proton_density,\n",
    "        STD_AVG_PROTON_DENSITY = std_avg_proton_density,\n",
    "        MEAN_AVG_SYM_H = mean_avg_sym_h,\n",
    "        STD_AVG_SYM_H = std_avg_sym_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
