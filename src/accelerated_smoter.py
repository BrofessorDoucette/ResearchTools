import numpy as np


def undersample(X, y, size, random_state=None):
    """
    Randomly undersample a dataset (X, y), and return a smaller dataset (X_new, y_new).

    Parameters
    ------------
    X : array-like or sparse matrix
        Features of the data with shape (n_samples, n_features)
    y : array-like
        The target values
    size : int
        Number of samples in new undersampled dataset.
    random_state : int or None, optional (default=None)
        If int, random_state is the seed used by the random number generator. If None,
        the random number generator is the RandomState instance used by np.random

    Returns
    ----------
    [X_new, y_new] : list
        List contanining features (X_new) and target values (y_new) after undersampling.
    """

    X, y = np.asarray(X), np.squeeze(np.asarray(y))
    assert len(X) == len(y), "X and y must be of the same length."
    if size >= len(y):
        raise ValueError("size must be smaller than the length of y")
    np.random.seed(random_state)
    new_indices = np.random.choice(range(len(y)), size, replace=False)
    X_new, y_new = X[new_indices, :], y[new_indices]
    return [X_new, y_new]


def smoter_interpolate(X, y, knn, k, size, nominal=None, random_state=None):
    """
    Generate new cases by interpolating between cases in the data and a randomly
    selected nearest neighbor. For nominal features, random selection is carried out,
    rather than interpolation.

    Parameters
    ------------
    X : array-like or sparse matrix
        Features of the data with shape (n_samples, n_features)
    y : array-like
        The target values
    k : int
        Number of nearest neighbors to use in generating synthetic cases by interpolation
    size : int
        Number of new cases to generate
    nominal : ndarray (default=None)
        Column indices of nominal features. If None, then all features are continuous
    random_state : int or None, optional (default=None)
        If int, random_state is the seed used by the random number generator. If None,
        the random number generator is the RandomState instance used by np.random.

    Returns
    --------
    [X_new, y_new] : list
        List contanining features (X_new) and target values (y_new) of new cases generated.
        Dimensions of X_new and y_new are the same as X and y, respectively.

    """

    X, y = np.asarray(X), np.squeeze(np.asarray(y))
    assert len(X) == len(y), "X and y must be of the same length."
    neighbor_indices = knn.query(X, k=k)  # Get indices of k nearest neighbors
    print(neighbor_indices)
    return 0
    np.random.seed(seed=random_state)
    sample_indices = np.random.choice(range(len(y)), size, replace=True)
    X_new, y_new = [], []

    for i in sample_indices:
        # Get case and nearest neighbor
        X_case, y_case = X[i, :], y[i]
        neighbor = np.random.choice(neighbor_indices[i, :])
        X_neighbor, y_neighbor = X[neighbor, :], y[neighbor]

        # Generate synthetic case by interpolation
        rand = np.random.rand() * np.ones_like(X_case)

        if nominal is not None:
            rand = [
                np.random.choice([0, 1]) if x in nominal else rand[x] for x in range(len(rand))
            ]  # Random selection for nominal features, rather than interpolation
            rand = np.asarray(rand)
        diff = (X_case - X_neighbor) * rand
        X_new_case = X_neighbor + diff
        d1 = np.linalg.norm(X_new_case - X_case)
        d2 = np.linalg.norm(X_new_case - X_neighbor)
        y_new_case = (d2 * y_case + d1 * y_neighbor) / (
            d2 + d1 + 1e-10
        )  # Add 1e-10  to avoid division by zero
        X_new.append(X_new_case)
        y_new.append(y_new_case)

    X_new = np.array(X_new)
    y_new = np.array(y_new)

    return [X_new, y_new]


def oversample(
    X, y, size, method, knn, k=None, delta=None, relevance=None, nominal=None, random_state=None
):
    """
    Randomly oversample a dataset (X, y) and return a larger dataset (X_new, y_new)
    according to specified method.

    Parameters
    -------------
    X : array-like or sparse matrix
        Features of the data with shape (n_samples, n_features)
    y : array-like
        The target values
    size : int
        Number of samples in new oversampled dataset.
    method : str, {'random_oversample' | 'smoter' | 'gaussian' | 'wercs' | 'wercs-gn'}
        Method for generating new samples.

        If 'random_oversample', samples are duplicated.

        If 'smoter', new synthetic samples are generated by interpolation with the SMOTER
        algorithm.

        If 'gaussian', new synthetic samples are generated by addition of Gaussian noise.

        If 'wercs', relevance values are used as weights to select values for duplication.

        If 'wercs-gn', values are selected with relevance values as weights and then
        Gaussian noise is added.

    k : int (default=None)
        Number of nearest neighbors to use in generating synthetic cases by interpolation.
        Must be specified if method is 'smoter'.
    delta : float (default=None)
        Value that determines the magnitude of Gaussian noise added. Must be specified if
        method is 'gaussian'
    relevance : array_like (default=None)
        Values ranging from 0 to 1 that indicate the relevance of target values. Must be
        specified if method is 'wercs' or 'wercs-gn'
    nominal : ndarray (default=None)
        Column indices of nominal features. If None, then all features are continuous.
    random_state : int or None, optional (default=None)
        If int, random_state is the seed used by the random number generator. If None,
        the random number generator is the RandomState instance used by np.random

    Returns
    ----------
    [X_new, y_new] : list
        List contanining features (X_new) and target values (y_new) of new samples after
        oversampling.
    """

    # Prepare data
    X, y = np.asarray(X), np.squeeze(np.asarray(y))
    assert len(X) == len(y), "X and y must be of the same length."
    moresize = int(size - len(y))
    if moresize <= 0:
        raise ValueError("size must be larger than the length of y")

    # Generate extra samples for oversampling
    np.random.seed(seed=random_state)
    if method == "smoter":
        if k is None:
            raise ValueError("Must specify k if method is 'smoter'")
        [X_more, y_more] = smoter_interpolate(
            X, y, knn, k, size=moresize, nominal=nominal, random_state=random_state
        )

    # Combine old dataset with extrasamples
    X_new = np.append(X, X_more, axis=0)
    y_new = np.append(y, y_more, axis=0)

    return [X_new, y_new]


def split_domains(X, y, relevance, relevance_threshold):
    """
    Split a dataset (X,y) into rare and normal domains according to the relevance of the
    target values. Target values with relevance values below the relevance threshold
    form the normal domain, while other target values form the rare domain.

    Parameters
    -------------
    X : array-like or sparse matrix
        Features of the data with shape (n_samples, n_features)
    y : array-like
        The target values
    relevance : array_like
        Values ranging from 0 to 1 that indicate the relevance of target values.
    relevance_threshold : float
        Threshold of relevance for forming rare and normal domains. Target values with
        relevance less than relevance_threshold form the normal domain, while target
        values with relevance greater than or equal to relevance_threshold form the rare
        domain.

    Returns
    -----------
    [X_norm, y_norm, X_rare, y_rare] : list
        List containing features (X_norm, X_rare) and target values (y_norm, y_rare) of
        the normal and rare domains.
    """

    X, y = np.asarray(X), np.squeeze(np.asarray(y))
    relevance = np.squeeze(np.asarray(relevance))
    assert len(X) == len(y) == len(relevance), "X, y, and relevance must have the same " "length"
    rare_indices = np.where(relevance >= relevance_threshold)[0]
    norm_indices = np.where(relevance < relevance_threshold)[0]
    assert len(rare_indices) < len(norm_indices), (
        "Rare domain must be smaller than "
        "normal domain. Adjust your relevance values or relevance threshold so "
        "that the there are fewer samples in the rare domain."
    )
    X_rare, y_rare = X[rare_indices, :], y[rare_indices]
    X_norm, y_norm = X[norm_indices, :], y[norm_indices]

    return [X_norm, y_norm, X_rare, y_rare]


def smoter(
    X,
    y,
    relevance,
    knn,
    relevance_threshold=0.5,
    k=5,
    over="balance",
    under=None,
    nominal=None,
    random_state=None,
):
    """
    Resample imbalanced dataset with the SMOTER algorithm. The dataset is split into a
    rare normal domain using relevance values. Target values with relevance below the
    relevance threshold form the normal domain, and other target values form the rare
    domain. The rare domain is oversampled by interpolating between samples, and the
    normal domain is undersampled.

    Parameters
    ------------
    X : array-like or sparse matrix
        Features of the data with shape (n_samples, n_features)
    y : array-like
        The target values
    relevance : 1d array-like
        Values ranging from 0 to 1 that indicate the relevance of target values.
    relevance_threshold : float (default=0.5)
        Threshold of relevance for forming rare and normal domains. Target values with
        relevance less than relevance_threshold form the normal domain, while target
        values with relevance greater than or equal to relevance_threshold form the rare
        domain.
    k : int (default=5)
        Number of nearest neighbors to use in generating synthetic cases by interpolation.
    over : float or str, {'balance' | 'extreme' | 'average'} (default='balance')
        Value that determines the amount of oversampling. If float, over is the fraction
        of new rare samples generated in oversampling.

        Otherwise, if string, over indicates the amount of both oversampling and
        undersampling. If 'balance', the rare domain is oversampled and the normal domain
        is undersampled so that they are equal in size.

        If 'extreme', oversampling and undersampling are done so that the ratio of the
        sizes of rare domain to normal domain is inverted.

        If 'average' the extent of oversampling and undersampling is intermediate between
        'balance' and 'extreme'.
    under : float (default=None)
        Value that determines the amount of undersampling. Should only be specified if
        over is float. One-third of normal samples are removed if under=0.33.
    nominal : ndarray (default=None)
        Column indices of nominal features. If None, then all features are continuous.
    random_state : int or None, optional (default=None)
        If int, random_state is the seed used by the random number generator. If None,
        the random number generator is the RandomState instance used by np.random.

    Returns
    ---------
    [X_new, y_new] : list
        List contanining features (X_new) and target values (y_new) of resampled dataset
        (both normal and rare samples).

    References
    -----------
    ..  [1] Torgo, L. et al (2015). Resampling strategies for regression.
        [2] Branco, P., Torgo, L., and Ribeiro, R.P. (2019). Pre-processing approaches for
        imbalanced distributions in regression.
    """

    # Split data into rare and normal dormains
    X, y = np.asarray(X), np.squeeze(np.asarray(y))
    relevance = np.squeeze(np.asarray(relevance))
    [X_norm, y_norm, X_rare, y_rare] = split_domains(X, y, relevance, relevance_threshold)
    norm_size, rare_size = len(y_norm), len(y_rare)

    # Determine new sizes for rare and normal domains after oversampling
    if type(over) == float:
        assert type(under) == float, "under must also be a float if over is a float"
        assert 0 <= under <= 1, "under must be between 0 and 1"
        assert over >= 0, "over must be non-negative"
        new_rare_size = int((1 + over) * rare_size)
        new_norm_size = int((1 - under) * norm_size)
    elif over == "balance":
        new_rare_size = new_norm_size = int((norm_size + rare_size) / 2)
    elif over == "extreme":
        new_rare_size, new_norm_size = norm_size, rare_size
    elif over == "average":
        new_rare_size = int(((norm_size + rare_size) / 2 + norm_size) / 2)
        new_norm_size = int(((norm_size + rare_size) / 2 + rare_size) / 2)
    else:
        raise ValueError(
            "Incorrect value of over, must be a float or  " "'balance', 'extreme', or 'average'"
        )

    # Oversample rare domain
    y_median = np.median(y)
    low_indices = np.where(y_rare < y_median)[0]
    high_indices = np.where(y_rare >= y_median)[0]

    # First oversample low rare cases
    if len(low_indices) != 0:
        size = int(len(low_indices) / rare_size * new_rare_size)
        X_low_rare, y_low_rare = oversample(
            X_rare[low_indices, :],
            y_rare[low_indices],
            size=size,
            method="smoter",
            knn=knn,
            k=k,
            relevance=relevance,
            nominal=nominal,
            random_state=random_state,
        )

    # Then do high rare cases
    if len(high_indices) != 0:
        size = int(len(high_indices) / rare_size * new_rare_size)
        X_high_rare, y_high_rare = oversample(
            X_rare[high_indices],
            y_rare[high_indices],
            size=size,
            method="smoter",
            knn=knn,
            k=k,
            relevance=relevance,
            nominal=nominal,
            random_state=random_state,
        )

    # Combine oversampled low and high rare cases
    if min(len(low_indices), len(high_indices)) != 0:
        X_rare_new = np.append(X_low_rare, X_high_rare, axis=0)
        y_rare_new = np.append(y_low_rare, y_high_rare, axis=0)
    elif len(low_indices) == 0:
        X_rare_new = X_high_rare
        y_rare_new = y_high_rare
    elif len(high_indices) == 0:
        X_rare_new = X_low_rare
        y_rare_new = y_low_rare

    # Undersample normal cases
    X_norm_new, y_norm_new = undersample(
        X_norm, y_norm, size=new_norm_size, random_state=random_state
    )

    # Combine resampled rare and normal cases
    X_new = np.append(X_rare_new, X_norm_new, axis=0)
    y_new = np.append(y_rare_new, y_norm_new, axis=0)

    return (X_new, y_new)
